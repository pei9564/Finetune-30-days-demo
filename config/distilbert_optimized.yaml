experiment_name: "distilbert_optimized_experiment"  # 優化後的 DistilBERT 實驗名稱

model:
  name: "distilbert-base-uncased"  # 使用 DistilBERT 模型
  num_labels: 2

lora:
  r: 4  # 減少 LoRA 秩以提高速度
  lora_alpha: 8  # 減少 alpha
  target_modules: ["q_lin", "v_lin"]
  lora_dropout: 0.05  # 減少 dropout
  bias: "none"
  task_type: "SEQ_CLS"

data:
  dataset_name: "glue"
  dataset_config: "sst2"
  train_samples: 500
  eval_samples: 100
  max_length: 128
  validation_rules:
    min_text_length: 5
    max_text_length: 500
    allow_empty: false
    remove_html: true

training:
  output_dir: "./results"
  eval_strategy: "epoch"
  learning_rate: 2.0e-3  # 增加學習率以加快收斂
  per_device_train_batch_size: 8  # 增加批次大小以提高吞吐量
  num_train_epochs: 1  # 保持單輪訓練
  logging_steps: 10
  device: null

system:
  log_file: "logs/training_progress.log"
  save_config: true
