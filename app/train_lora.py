import logging
import os

import evaluate
import numpy as np
import torch
from datasets import load_dataset
from logger_config import setup_logger
from peft import LoraConfig, get_peft_model
from transformers import (
    AutoModelForSequenceClassification,
    AutoTokenizer,
    Trainer,
    TrainerCallback,
    TrainingArguments,
)

logger = setup_logger()


# ===== è¨“ç·´é€²åº¦è¨˜éŒ„ Callback =====
class TrainingProgressCallback(TrainerCallback):
    """è¨“ç·´é€²åº¦è¨˜éŒ„ callbackï¼Œå°‡è¨“ç·´æŒ‡æ¨™ä¿å­˜åˆ°æ–‡ä»¶"""

    def __init__(self, log_file):
        super().__init__()
        self.log_file = log_file

        # å‰µå»º logger
        self.logger = logging.getLogger("training_progress")
        self.logger.setLevel(logging.INFO)

        # æ¸…é™¤ç¾æœ‰ handlers
        for handler in self.logger.handlers[:]:
            self.logger.removeHandler(handler)

        # åªè¼¸å‡ºåˆ°æ–‡ä»¶ï¼Œé¿å…èˆ‡çµ‚ç«¯é¡¯ç¤ºè¡çª
        file_handler = logging.FileHandler(log_file, mode="a")
        file_handler.setLevel(logging.INFO)
        file_handler.setFormatter(
            logging.Formatter("%(asctime)s - PROGRESS - %(message)s")
        )

        self.logger.addHandler(file_handler)

    def on_log(self, args, state, control, logs=None, **kwargs):
        """è¨˜éŒ„è¨“ç·´é€²åº¦"""
        if logs:
            metrics = []

            # æ”¶é›†è¨“ç·´æŒ‡æ¨™
            if "loss" in logs:
                metrics.append(f"loss={logs['loss']:.4f}")
            if "learning_rate" in logs:
                metrics.append(f"lr={logs['learning_rate']:.6f}")
            if "epoch" in logs:
                metrics.append(f"epoch={logs['epoch']:.2f}")
            if "eval_loss" in logs:
                metrics.append(f"eval_loss={logs['eval_loss']:.4f}")
            if "eval_accuracy" in logs:
                metrics.append(f"accuracy={logs['eval_accuracy']:.4f}")

            # åˆä½µæŒ‡æ¨™åˆ°ä¸€è¡Œ
            if metrics:
                message = f"Step {state.global_step}: {' | '.join(metrics)}"
                self.logger.info(message)

    def on_evaluate(self, args, state, control, metrics=None, **kwargs):
        """è¨˜éŒ„è©•ä¼°çµæœ"""
        if metrics:
            eval_metrics = []

            for key, value in metrics.items():
                if isinstance(value, (int, float)):
                    eval_metrics.append(f"{key}={value:.4f}")
                else:
                    eval_metrics.append(f"{key}={value}")

            if eval_metrics:
                message = f"Evaluation: {' | '.join(eval_metrics)}"
                self.logger.info(message)


# ===== 1. è‡ªå‹•åµæ¸¬è£ç½® =====
if torch.cuda.is_available():
    device = torch.device("cuda")
    logger.info("ğŸš€ ä½¿ç”¨ CUDA GPU åŠ é€Ÿ")
elif torch.backends.mps.is_available():
    device = torch.device("mps")
    logger.info("ğŸš€ ä½¿ç”¨ MPS åŠ é€Ÿï¼ˆApple Siliconï¼‰")
else:
    device = torch.device("cpu")
    logger.info("âš ï¸ æœªæª¢æ¸¬åˆ° GPUï¼Œä½¿ç”¨ CPU æ¨¡å¼")

logger.info(f"âœ… ä½¿ç”¨è£ç½®: {device}")

# ===== 2. è¼‰å…¥æ¨¡å‹èˆ‡ tokenizer =====
logger.info("ğŸ“¥ è¼‰å…¥æ¨¡å‹èˆ‡ tokenizer...")
model_name = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2).to(
    device
)
logger.info(f"âœ… æ¨¡å‹è¼‰å…¥å®Œæˆ: {model_name}")

# ===== 3. è¼‰å…¥å°å‹è³‡æ–™é›† =====
logger.info("ğŸ“Š è¼‰å…¥è³‡æ–™é›†...")
dataset = load_dataset("glue", "sst2")
train_small = dataset["train"].select(range(500))
eval_small = dataset["validation"].select(range(100))
logger.info(f"âœ… è¨“ç·´è³‡æ–™: {len(train_small)} ç­†")
logger.info(f"âœ… é©—è­‰è³‡æ–™: {len(eval_small)} ç­†")


def tokenize(batch):
    return tokenizer(
        batch["sentence"], padding="max_length", truncation=True, max_length=128
    )


logger.info("ğŸ”„ è™•ç†è³‡æ–™...")
train_dataset = train_small.map(tokenize, batched=True)
eval_dataset = eval_small.map(tokenize, batched=True)
logger.info("âœ… è³‡æ–™è™•ç†å®Œæˆ")

# ===== 4. LoRA é…ç½® =====
logger.info("ğŸ”§ é…ç½® LoRA...")
lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["q_lin", "v_lin"],
    lora_dropout=0.1,
    bias="none",
    task_type="SEQ_CLS",
)
model = get_peft_model(model, lora_config).to(device)
logger.info("âœ… LoRA é…ç½®å®Œæˆ")

# é¡¯ç¤ºå¯è¨“ç·´åƒæ•¸æ•¸é‡
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
total_params = sum(p.numel() for p in model.parameters())
logger.info(
    f"ğŸ“Š å¯è¨“ç·´åƒæ•¸: {trainable_params:,} / {total_params:,} ({trainable_params / total_params * 100:.2f}%)"
)

# ===== 5. è©•ä¼°æ–¹æ³• =====
logger.info("ğŸ“ˆ è¨­ç½®è©•ä¼°æ–¹æ³•...")
metric = evaluate.load("accuracy")


def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=-1)
    return metric.compute(predictions=preds, references=labels)


# ===== 6. è¨“ç·´åƒæ•¸ =====
logger.info("âš™ï¸ è¨­ç½®è¨“ç·´åƒæ•¸...")
training_args = TrainingArguments(
    output_dir="./results",
    eval_strategy="epoch",
    learning_rate=5e-4,
    per_device_train_batch_size=2,
    num_train_epochs=1,
    logging_steps=10,
    report_to=None,
)

logger.info(f"ğŸ“ è¨“ç·´åƒæ•¸:")
logger.info(f"   - å­¸ç¿’ç‡: {training_args.learning_rate}")
logger.info(f"   - æ‰¹æ¬¡å¤§å°: {training_args.per_device_train_batch_size}")
logger.info(f"   - è¨“ç·´è¼ªæ•¸: {training_args.num_train_epochs}")
logger.info(f"   - è¨˜éŒ„é »ç‡: æ¯ {training_args.logging_steps} æ­¥")

# ===== 7. å»ºç«‹ Trainer ä¸¦è¨“ç·´ =====
logger.info("ğŸš€ é–‹å§‹è¨“ç·´...")
logger.info("=" * 50)

# å‰µå»ºè‡ªå®šç¾© callback ä¾†è¨˜éŒ„è¨“ç·´é€²åº¦ï¼ˆåªè¨˜éŒ„åˆ°æ–‡ä»¶ï¼Œä¸åœ¨çµ‚ç«¯é¡¯ç¤ºï¼‰
progress_callback = TrainingProgressCallback("logs/local_training.log")

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    compute_metrics=compute_metrics,
    callbacks=[progress_callback],  # æ·»åŠ è‡ªå®šç¾© callback
)

# è¨“ç·´
train_result = trainer.train()

logger.info("ğŸ‰ è¨“ç·´å®Œæˆï¼")
logger.info("=" * 50)

# è©•ä¼°
logger.info("ğŸ“Š è©•ä¼°æ¨¡å‹...")
eval_result = trainer.evaluate()
logger.info(f"âœ… é©—è­‰æº–ç¢ºç‡: {eval_result['eval_accuracy']:.4f}")

# ä¿å­˜æ¨¡å‹
logger.info("ğŸ’¾ ä¿å­˜æ¨¡å‹...")
trainer.save_model("./results/final_model")
logger.info("âœ… æ¨¡å‹å·²ä¿å­˜åˆ° ./results/final_model")

# è¨“ç·´ç¸½çµ
logger.info("ğŸ¯ è¨“ç·´ç¸½çµ:")
logger.info(f"   - ç¸½è¨“ç·´æ­¥æ•¸: {train_result.global_step}")
logger.info(f"   - ç¸½è¨“ç·´æ™‚é–“: {train_result.metrics['train_runtime']:.2f} ç§’")
logger.info(f"   - é©—è­‰æº–ç¢ºç‡: {eval_result['eval_accuracy']:.4f}")
logger.info(f"   - æ¨¡å‹ä¿å­˜ä½ç½®: ./results/final_model")
