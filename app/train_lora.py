import logging
import evaluate
import numpy as np
import torch
from datasets import load_dataset
from logger_config import setup_logger
from peft import LoraConfig, get_peft_model
from transformers import (
    AutoModelForSequenceClassification,
    AutoTokenizer,
    Trainer,
    TrainerCallback,
    TrainingArguments,
)

from data_management import (
    DataValidator,
    DataVersionManager,
    analyze_distribution,
    get_data_summary,
)

logger = setup_logger()


# ===== è¨“ç·´é€²åº¦è¨˜éŒ„ Callback =====
class TrainingProgressCallback(TrainerCallback):
    """è¨“ç·´é€²åº¦è¨˜éŒ„ callbackï¼Œå°‡è¨“ç·´æŒ‡æ¨™ä¿å­˜åˆ°æ–‡ä»¶"""

    def __init__(self, log_file):
        super().__init__()
        self.log_file = log_file

        # å‰µå»º logger
        self.logger = logging.getLogger("training_progress")
        self.logger.setLevel(logging.INFO)

        # æ¸…é™¤ç¾æœ‰ handlers
        for handler in self.logger.handlers[:]:
            self.logger.removeHandler(handler)

        # åªè¼¸å‡ºåˆ°æ–‡ä»¶ï¼Œé¿å…èˆ‡çµ‚ç«¯é¡¯ç¤ºè¡çª
        file_handler = logging.FileHandler(log_file, mode="a")
        file_handler.setLevel(logging.INFO)
        file_handler.setFormatter(
            logging.Formatter("%(asctime)s - PROGRESS - %(message)s")
        )

        self.logger.addHandler(file_handler)

    def on_log(self, args, state, control, logs=None, **kwargs):
        """è¨˜éŒ„è¨“ç·´é€²åº¦"""
        if logs:
            metrics = []

            # æ”¶é›†è¨“ç·´æŒ‡æ¨™
            if "loss" in logs:
                metrics.append(f"loss={logs['loss']:.4f}")
            if "learning_rate" in logs:
                metrics.append(f"lr={logs['learning_rate']:.6f}")
            if "epoch" in logs:
                metrics.append(f"epoch={logs['epoch']:.2f}")
            if "eval_loss" in logs:
                metrics.append(f"eval_loss={logs['eval_loss']:.4f}")
            if "eval_accuracy" in logs:
                metrics.append(f"accuracy={logs['eval_accuracy']:.4f}")

            # åˆä½µæŒ‡æ¨™åˆ°ä¸€è¡Œ
            if metrics:
                message = f"Step {state.global_step}: {' | '.join(metrics)}"
                self.logger.info(message)

    def on_evaluate(self, args, state, control, metrics=None, **kwargs):
        """è¨˜éŒ„è©•ä¼°çµæœ"""
        if metrics:
            eval_metrics = []

            for key, value in metrics.items():
                if isinstance(value, (int, float)):
                    eval_metrics.append(f"{key}={value:.4f}")
                else:
                    eval_metrics.append(f"{key}={value}")

            if eval_metrics:
                message = f"Evaluation: {' | '.join(eval_metrics)}"
                self.logger.info(message)


# ===== 1. è‡ªå‹•åµæ¸¬è£ç½® =====
if torch.cuda.is_available():
    device = torch.device("cuda")
    logger.info("ğŸš€ ä½¿ç”¨ CUDA GPU åŠ é€Ÿ")
elif torch.backends.mps.is_available():
    device = torch.device("mps")
    logger.info("ğŸš€ ä½¿ç”¨ MPS åŠ é€Ÿï¼ˆApple Siliconï¼‰")
else:
    device = torch.device("cpu")
    logger.info("âš ï¸ æœªæª¢æ¸¬åˆ° GPUï¼Œä½¿ç”¨ CPU æ¨¡å¼")


# ===== 2. è¼‰å…¥æ¨¡å‹èˆ‡ tokenizer =====
logger.info("ğŸ“¥ è¼‰å…¥æ¨¡å‹èˆ‡ tokenizer...")
model_name = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2).to(
    device
)
logger.info(f"âœ… æ¨¡å‹è¼‰å…¥å®Œæˆ: {model_name}")

# ===== 3. è¼‰å…¥å°å‹è³‡æ–™é›† =====
logger.info("ğŸ“Š è¼‰å…¥è³‡æ–™é›†...")
dataset = load_dataset("glue", "sst2")
train_small = dataset["train"].select(range(500))
eval_small = dataset["validation"].select(range(100))
logger.info(f"   - è¨“ç·´è³‡æ–™: {len(train_small)} ç­†")
logger.info(f"   - é©—è­‰è³‡æ–™: {len(eval_small)} ç­†")

# ===== è³‡æ–™ç®¡ç†èˆ‡ç‰ˆæœ¬æ§åˆ¶ =====

logger.info("ğŸ“‹ é€²è¡Œè³‡æ–™åˆ†æèˆ‡ç®¡ç†...")

# 1. è³‡æ–™æ‘˜è¦
summary = get_data_summary(train_small)
logger.info(f"ğŸ“Š è³‡æ–™æ‘˜è¦:")
logger.info(f"   - ç‰¹å¾µæ•¸: {summary['num_features']}")
logger.info(f"   - ç‰¹å¾µåç¨±: {summary['feature_names']}")

# 2. åˆ†å¸ƒåˆ†æ
distribution_analysis = analyze_distribution(train_small)
logger.info(f"ğŸ“Š é¡åˆ¥åˆ†å¸ƒ:")
logger.info(f"   - é¡åˆ¥æ•¸: {distribution_analysis['num_classes']}")
logger.info(f"   - å„é¡åˆ¥æ•¸é‡: {distribution_analysis['label_counts']}")
logger.info(f"   - ä¸å¹³è¡¡æ¯”ä¾‹: {distribution_analysis['imbalance_ratio']:.2f}:1")
logger.info(f"   - æ˜¯å¦å¹³è¡¡: {'âœ…' if distribution_analysis['is_balanced'] else 'âŒ'}")

# 3. è³‡æ–™é©—è­‰
validator = DataValidator(logger)
validator.set_validation_rules(
    {
        "min_text_length": 5,
        "max_text_length": 500,
        "allow_empty": False,
        "remove_html": True,
    }
)

validation_report = validator.validate_dataset(train_small, ["sentence"])
total_issues = sum(
    len(issue_list) for issue_list in validation_report["issues"].values()
)

if total_issues > 0:
    logger.warning(f"âš ï¸ ç™¼ç¾ {total_issues} å€‹è³‡æ–™å•é¡Œ")
    # æ¸…ç†è³‡æ–™
    train_small = validator.clean_dataset(train_small, ["sentence"], validation_report)
    logger.info(f"ğŸ§¹ è³‡æ–™æ¸…ç†å®Œæˆï¼Œå‰©é¤˜ {len(train_small)} ç­†è¨“ç·´è³‡æ–™")
else:
    logger.info("âœ… è³‡æ–™é©—è­‰é€šéï¼Œç„¡å•é¡Œç™¼ç¾")

# 4. ç‰ˆæœ¬ç®¡ç†
try:
    version_manager = DataVersionManager(logger=logger)

    # æª¢æŸ¥æ˜¯å¦å·²æœ‰ç•¶å‰ç‰ˆæœ¬
    current_version = version_manager.get_current_version()
    if current_version:
        logger.info(f"ğŸ“¦ ç•¶å‰è³‡æ–™ç‰ˆæœ¬: {current_version}")
    else:
        # å‰µå»ºç¬¬ä¸€å€‹ç‰ˆæœ¬
        version_name = f"sst2_train_{len(train_small)}samples"
        version_manager.create_version(
            train_small,
            version_name,
            description=f"SST-2 è¨“ç·´é›†ï¼Œç¶“éæ¸…ç†ï¼Œ{len(train_small)} ç­†è³‡æ–™",
            cleaning_strategy="ç§»é™¤ç©ºå€¼ã€HTMLæ¨™ç±¤æ¸…ç†ã€é‡è¤‡è³‡æ–™ç§»é™¤",
            source_info={
                "dataset": "glue/sst2",
                "split": "train",
                "original_samples": 500,
                "cleaned_samples": len(train_small),
            },
        )
        logger.info(f"ğŸ“¦ å‰µå»ºè³‡æ–™ç‰ˆæœ¬: {version_name}")

except Exception as e:
    logger.warning(f"âš ï¸ ç‰ˆæœ¬ç®¡ç†å¤±æ•—: {e}")

logger.info("=" * 50)


def tokenize(batch):
    return tokenizer(
        batch["sentence"], padding="max_length", truncation=True, max_length=128
    )


train_dataset = train_small.map(tokenize, batched=True)
eval_dataset = eval_small.map(tokenize, batched=True)
logger.info("âœ… è¨“ç·´å’Œé©—è­‰è³‡æ–™é›†è™•ç†å®Œæˆ")

# ===== 4. LoRA é…ç½® =====
lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["q_lin", "v_lin"],
    lora_dropout=0.1,
    bias="none",
    task_type="SEQ_CLS",
)
model = get_peft_model(model, lora_config).to(device)
logger.info("âœ… LoRA é…ç½®å®Œæˆ")

# é¡¯ç¤ºå¯è¨“ç·´åƒæ•¸æ•¸é‡
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
total_params = sum(p.numel() for p in model.parameters())
logger.info(
    f"ğŸ“Š å¯è¨“ç·´åƒæ•¸: {trainable_params:,} / {total_params:,} ({trainable_params / total_params * 100:.2f}%)"
)

# ===== 5. è©•ä¼°æ–¹æ³• =====
logger.info("ğŸ“ˆ è¨­ç½®è©•ä¼°æ–¹æ³•...")
metric = evaluate.load("accuracy")


def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=-1)
    return metric.compute(predictions=preds, references=labels)


# ===== 6. è¨“ç·´åƒæ•¸ =====
logger.info("âš™ï¸ è¨­ç½®è¨“ç·´åƒæ•¸...")
training_args = TrainingArguments(
    output_dir="./results",
    eval_strategy="epoch",
    learning_rate=5e-4,
    per_device_train_batch_size=2,
    num_train_epochs=1,
    logging_steps=10,
    report_to=None,
)

logger.info(f"ğŸ“ è¨“ç·´åƒæ•¸:")
logger.info(f"   - å­¸ç¿’ç‡: {training_args.learning_rate}")
logger.info(f"   - æ‰¹æ¬¡å¤§å°: {training_args.per_device_train_batch_size}")
logger.info(f"   - è¨“ç·´è¼ªæ•¸: {training_args.num_train_epochs}")
logger.info(f"   - è¨˜éŒ„é »ç‡: æ¯ {training_args.logging_steps} æ­¥")

# ===== 7. å»ºç«‹ Trainer ä¸¦è¨“ç·´ =====
logger.info("ğŸš€ é–‹å§‹è¨“ç·´...")
logger.info("=" * 50)

# å‰µå»ºè‡ªå®šç¾© callback ä¾†è¨˜éŒ„è¨“ç·´é€²åº¦ï¼ˆåªè¨˜éŒ„åˆ°æ–‡ä»¶ï¼Œä¸åœ¨çµ‚ç«¯é¡¯ç¤ºï¼‰
progress_callback = TrainingProgressCallback("logs/local_training.log")

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    compute_metrics=compute_metrics,
    callbacks=[progress_callback],  # æ·»åŠ è‡ªå®šç¾© callback
)

# è¨“ç·´
train_result = trainer.train()

logger.info("ğŸ‰ è¨“ç·´å®Œæˆï¼")
logger.info("=" * 50)

# è©•ä¼°
logger.info("ğŸ“Š è©•ä¼°æ¨¡å‹...")
eval_result = trainer.evaluate()
logger.info(f"âœ… é©—è­‰æº–ç¢ºç‡: {eval_result['eval_accuracy']:.4f}")

# ä¿å­˜æ¨¡å‹
logger.info("ğŸ’¾ ä¿å­˜æ¨¡å‹...")
trainer.save_model("./results/final_model")
logger.info("âœ… æ¨¡å‹å·²ä¿å­˜åˆ° ./results/final_model")

# è¨“ç·´ç¸½çµ
logger.info("ğŸ¯ è¨“ç·´ç¸½çµ:")
logger.info(f"   - ç¸½è¨“ç·´æ­¥æ•¸: {train_result.global_step}")
logger.info(f"   - ç¸½è¨“ç·´æ™‚é–“: {train_result.metrics['train_runtime']:.2f} ç§’")
logger.info(f"   - é©—è­‰æº–ç¢ºç‡: {eval_result['eval_accuracy']:.4f}")
logger.info(f"   - æ¨¡å‹ä¿å­˜ä½ç½®: ./results/final_model")
