"""
è¨“ç·´ä¸»æµç¨‹ç›¸é—œåŠŸèƒ½
"""

import logging
import os
import time
from datetime import datetime
from typing import Dict, Optional, Tuple

import mlflow
import psutil
import torch
from datasets import Dataset, load_dataset
from peft import LoraConfig, get_peft_model
from transformers import (
    AutoModelForSequenceClassification,
    AutoTokenizer,
    PreTrainedModel,
    PreTrainedTokenizer,
    Trainer,
    TrainingArguments,
)

from app.core.config import Config
from app.core.mlflow_config import init_mlflow
from app.models.model_registry import ModelCard, registry
from app.tools.artifact_utils import save_artifact
from app.train.evaluator import TrainingProgressCallback, compute_metrics

logger = logging.getLogger(__name__)

DEFAULT_TEST_SIZE = 0.1
DEFAULT_SEED = 42
MEMORY_WARNING_THRESHOLD = 0.9


def load_model_and_tokenizer(
    config: Config, device: str
) -> Tuple[PreTrainedModel, PreTrainedTokenizer]:
    """è¼‰å…¥æ¨¡å‹å’Œåˆ†è©å™¨

    Args:
        config: è¨“ç·´é…ç½®
        device: è¨­å‚™åç¨±

    Returns:
        tuple: (model, tokenizer) æ¨¡å‹å’Œåˆ†è©å™¨
    """
    logger.info(f"ğŸ¤– è¼‰å…¥æ¨¡å‹ {config.model.name}...")
    model = AutoModelForSequenceClassification.from_pretrained(
        config.model.name,
        num_labels=config.model.num_labels,
        trust_remote_code=True,
    )
    tokenizer = AutoTokenizer.from_pretrained(config.model.name)

    # ç§»å‹•æ¨¡å‹åˆ°æŒ‡å®šè¨­å‚™
    model = model.to(device)
    logger.info(f"âœ… æ¨¡å‹å·²è¼‰å…¥åˆ° {device}")

    return model, tokenizer


def setup_lora(config: Config, model: PreTrainedModel, device: str) -> PreTrainedModel:
    """è¨­ç½® LoRA é…ç½®

    Args:
        config: è¨“ç·´é…ç½®
        model: åŸºç¤æ¨¡å‹
        device: è¨­å‚™åç¨±

    Returns:
        PreTrainedModel: æ·»åŠ  LoRA å¾Œçš„æ¨¡å‹
    """
    logger.info("ğŸ”§ è¨­ç½® LoRA é…ç½®...")
    lora_config = LoraConfig(
        r=config.lora.r,
        lora_alpha=config.lora.lora_alpha,
        target_modules=config.lora.target_modules,
        lora_dropout=config.lora.lora_dropout,
        bias="none",
        task_type="SEQ_CLS",
    )

    # æ·»åŠ  LoRA é©é…å™¨
    model = get_peft_model(model, lora_config)
    model = model.to(device)

    # æ‰“å°åƒæ•¸çµ±è¨ˆ
    model.print_trainable_parameters()
    logger.info("âœ… LoRA é…ç½®å®Œæˆ")

    return model


def setup_device(config: Config) -> str:
    """è¨­ç½®è¨“ç·´è¨­å‚™

    Args:
        config: è¨“ç·´é…ç½®

    Returns:
        str: è¨­å‚™åç¨±
    """
    if config.training.device == "auto":
        if torch.cuda.is_available():
            device = "cuda"
        elif torch.backends.mps.is_available():
            device = "mps"
        else:
            device = "cpu"
    else:
        device = config.training.device

    logger.info(f"ğŸ’» ä½¿ç”¨è¨­å‚™: {device}")
    return device


def load_and_process_data(
    config: Config, tokenizer: PreTrainedTokenizer
) -> Tuple[Dataset, Dataset]:
    """è¼‰å…¥ä¸¦è™•ç†æ•¸æ“š

    Args:
        config: è¨“ç·´é…ç½®
        tokenizer: åˆ†è©å™¨

    Returns:
        tuple: (train_dataset, eval_dataset) è¨“ç·´é›†å’Œé©—è­‰é›†
    """
    logger.info("ğŸ“¦ è¼‰å…¥æ•¸æ“šé›†...")

    dataset = load_dataset(
        config.data.dataset_name,
        config.data.subset,
        split="train",
        trust_remote_code=True,
    )

    # éš¨æ©Ÿåˆ†å‰²è¨“ç·´é›†å’Œé©—è­‰é›†
    dataset = dataset.train_test_split(test_size=DEFAULT_TEST_SIZE, seed=DEFAULT_SEED)
    train_dataset = dataset["train"]
    eval_dataset = dataset["test"]

    # å¦‚æœæŒ‡å®šäº†è¨“ç·´æ¨£æœ¬æ•¸ï¼Œå‰‡åªä½¿ç”¨éƒ¨åˆ†æ•¸æ“š
    if config.training.train_samples:
        train_dataset = train_dataset.select(range(config.training.train_samples))

    logger.info(f"âœ… è¨“ç·´é›†å¤§å°: {len(train_dataset)}")
    logger.info(f"âœ… é©—è­‰é›†å¤§å°: {len(eval_dataset)}")

    return train_dataset, eval_dataset


def setup_training(
    config: Config,
    model: PreTrainedModel,
    train_dataset: Dataset,
    eval_dataset: Dataset,
    exp_dir: str,
) -> Trainer:
    """è¨­ç½®è¨“ç·´

    Args:
        config: è¨“ç·´é…ç½®
        model: æ¨¡å‹
        train_dataset: è¨“ç·´è³‡æ–™é›†
        eval_dataset: é©—è­‰è³‡æ–™é›†
        exp_dir: å¯¦é©—ç›®éŒ„ï¼ˆæœªä½¿ç”¨ï¼Œä¿ç•™ä»¥ç¶­æŒæ¥å£ä¸€è‡´æ€§ï¼‰

    Returns:
        Trainer: è¨“ç·´å™¨å¯¦ä¾‹
    """

    # è¨“ç·´åƒæ•¸
    logger.info("âš™ï¸ è¨­ç½®è¨“ç·´åƒæ•¸...")
    training_args = TrainingArguments(
        output_dir=config.training.output_dir,
        learning_rate=config.training.learning_rate,
        per_device_train_batch_size=config.training.per_device_train_batch_size,
        num_train_epochs=config.training.num_train_epochs,
        logging_steps=config.training.logging_steps,
        report_to=None,
        # Checkpoint ç›¸é—œé…ç½®
        save_strategy="epoch",  # æ¯å€‹ epoch ä¿å­˜ä¸€æ¬¡
        save_total_limit=None,  # ä¸é™åˆ¶ä¿å­˜æ•¸é‡ï¼Œç”± CheckpointManager ç®¡ç†
        eval_strategy="epoch",  # æ¯å€‹ epoch è©•ä¼°ä¸€æ¬¡
        load_best_model_at_end=True,  # è¨“ç·´çµæŸå¾Œè¼‰å…¥æœ€ä½³æ¨¡å‹
        metric_for_best_model="eval_accuracy",  # ä½¿ç”¨é©—è­‰æº–ç¢ºç‡é¸æ“‡æœ€ä½³æ¨¡å‹
        greater_is_better=True,  # æŒ‡æ¨™è¶Šå¤§è¶Šå¥½
    )

    # åˆä½µè¨“ç·´åƒæ•¸æ—¥èªŒ
    logger.info("ğŸ“ è¨“ç·´åƒæ•¸:")
    logger.info(f"   - å­¸ç¿’ç‡: {training_args.learning_rate}")
    logger.info(f"   - æ‰¹æ¬¡å¤§å°: {training_args.per_device_train_batch_size}")
    logger.info(f"   - è¨“ç·´è¼ªæ•¸: {training_args.num_train_epochs}")
    logger.info(f"   - è¨˜éŒ„é »ç‡: æ¯ {training_args.logging_steps} æ­¥")
    logger.info("   - ä¿å­˜ç­–ç•¥: æ¯å€‹ epoch")
    logger.info("   - è©•ä¼°ç­–ç•¥: æ¯å€‹ epoch")
    logger.info(f"   - è¼‰å…¥æœ€ä½³æ¨¡å‹: {training_args.load_best_model_at_end}")
    logger.info(f"   - è©•ä¼°æŒ‡æ¨™: {training_args.metric_for_best_model}")

    # å‰µå»ºè‡ªå®šç¾© callbackï¼Œä½¿ç”¨ artifacts ç›®éŒ„ä¸­çš„æ—¥èªŒæ–‡ä»¶
    log_file = os.path.join(str(config.training.output_dir), "logs.txt")
    # ç¢ºä¿æ—¥èªŒç›®éŒ„å­˜åœ¨
    os.makedirs(os.path.dirname(log_file), exist_ok=True)
    progress_callback = TrainingProgressCallback(log_file)

    # å‰µå»º Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        compute_metrics=compute_metrics,
        callbacks=[progress_callback],
    )

    return trainer


def train_and_evaluate(
    config: Config, trainer: Trainer
) -> Tuple[Dict, Optional[Dict], str]:
    """è¨“ç·´èˆ‡è©•ä¼°

    Args:
        config: è¨“ç·´é…ç½®
        trainer: è¨“ç·´å™¨å¯¦ä¾‹

    Returns:
        tuple: (train_result, eval_result, run_id) è¨“ç·´çµæœã€è©•ä¼°çµæœå’Œ MLflow run ID

    Raises:
        RuntimeError: ç•¶è¨˜æ†¶é«”ä¸è¶³æ™‚
    """
    # Initialize MLflow and start run
    mlflow_config = init_mlflow()

    with mlflow.start_run(
        experiment_id=mlflow_config["experiment_id"],
        run_name=f"{config.model.name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
    ) as run:
        logger.info("ğŸš€ é–‹å§‹è¨“ç·´...")
        logger.info("=" * 50)
        logger.info(f"MLflow å¯¦é©— ID: {mlflow_config['experiment_id']}")
        logger.info(f"MLflow Run ID: {run.info.run_id}")

        # Log parameters
        mlflow.log_params(
            {
                "model_name": config.model.name,
                "batch_size": config.training.per_device_train_batch_size,
                "learning_rate": config.training.learning_rate,
                "epochs": config.training.num_train_epochs,
                "device": str(trainer.args.device),
                "lora_r": config.lora.r,
                "lora_alpha": config.lora.lora_alpha,
                "lora_dropout": config.lora.lora_dropout,
            }
        )

        # æª¢æŸ¥åˆå§‹è¨˜æ†¶é«”ç‹€æ…‹
        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()
            initial_gpu_memory = torch.cuda.memory_allocated() / 1024**3  # GB
            logger.info(f"åˆå§‹ GPU è¨˜æ†¶é«”ä½¿ç”¨: {initial_gpu_memory:.2f}GB")

        try:
            # è¨“ç·´
            start_time = time.time()
            train_result = trainer.train()
            training_time = time.time() - start_time

            logger.info("ğŸ‰ è¨“ç·´å®Œæˆï¼")

            # Log training metrics
            mlflow.log_metrics(
                {
                    "training_time": training_time,
                    "total_steps": train_result.global_step,
                    "train_loss": train_result.metrics.get("train_loss", 0.0),
                    "train_runtime": train_result.metrics["train_runtime"],
                    "train_samples_per_second": train_result.metrics[
                        "train_samples_per_second"
                    ],
                }
            )

            # è¨˜éŒ„æœ€å¤§è¨˜æ†¶é«”ä½¿ç”¨é‡
            if torch.cuda.is_available():
                peak_gpu_memory = torch.cuda.max_memory_allocated() / 1024**3  # GB
                current_gpu_memory = torch.cuda.memory_allocated() / 1024**3  # GB
                logger.info(f"æœ€å¤§ GPU è¨˜æ†¶é«”ä½¿ç”¨: {peak_gpu_memory:.2f}GB")
                logger.info(f"ç•¶å‰ GPU è¨˜æ†¶é«”ä½¿ç”¨: {current_gpu_memory:.2f}GB")

                # æª¢æŸ¥æ˜¯å¦æ¥è¿‘è¨˜æ†¶é«”é™åˆ¶
                total_gpu_memory = (
                    torch.cuda.get_device_properties(0).total_memory / 1024**3
                )
                if (
                    peak_gpu_memory > total_gpu_memory * MEMORY_WARNING_THRESHOLD
                ):  # ä½¿ç”¨è¶…é 90% çš„è¨˜æ†¶é«”
                    logger.warning(
                        f"âš ï¸ GPU è¨˜æ†¶é«”ä½¿ç”¨ç‡éé«˜: {(peak_gpu_memory / total_gpu_memory) * 100:.1f}%"
                    )

            logger.info("=" * 50)

            # è©•ä¼°
            logger.info("ğŸ“Š è©•ä¼°æ¨¡å‹...")
            eval_result = trainer.evaluate()
            logger.info(f"âœ… é©—è­‰æº–ç¢ºç‡: {eval_result['eval_accuracy']:.4f}")

            # Log evaluation metrics
            mlflow.log_metrics(
                {
                    "eval_accuracy": eval_result["eval_accuracy"],
                    "eval_loss": eval_result["eval_loss"],
                }
            )

            # ä¿å­˜æ¨¡å‹åˆ°æœ¬åœ°
            output_dir = os.path.join(config.training.output_dir, "final_model")
            os.makedirs(output_dir, exist_ok=True)
            logger.info(f"ğŸ’¾ ä¿å­˜æ¨¡å‹åˆ° {output_dir}...")

            # ä¿å­˜åˆ°æœ¬åœ°
            trainer.save_model(output_dir)
            logger.info("âœ… æ¨¡å‹ä¿å­˜å®Œæˆ")

            save_artifact(output_dir, run.info.run_id)

            # è¨˜éŒ„æ¨¡å‹åˆ° MLflow
            model_name = config.model.name.split("/")[-1]  # Get base name without org
            logger.info("ğŸ“¦ è¨˜éŒ„æ¨¡å‹åˆ° MLflow...")

            # ä½¿ç”¨ mlflow.pytorch.save_model å…ˆä¿å­˜åˆ°è‡¨æ™‚ç›®éŒ„
            temp_model_dir = os.path.join(output_dir, "mlflow_model")
            os.makedirs(temp_model_dir, exist_ok=True)
            mlflow.pytorch.save_model(trainer.model, temp_model_dir)

            # ä½¿ç”¨ log_artifacts è¨˜éŒ„åˆ° MLflow
            mlflow.log_artifacts(temp_model_dir, "final_model")

            # è¨»å†Šæ¨¡å‹åˆ° MLflow Registry
            model_uri = f"runs:/{run.info.run_id}/final_model"
            logger.info("ğŸ“ è¨»å†Šæ¨¡å‹åˆ° MLflow Registry...")
            registered_model = mlflow.register_model(
                model_uri=model_uri, name=model_name
            )
            logger.info(
                f"âœ… æ¨¡å‹å·²è¨»å†Šåˆ° MLflow Registry: {model_name} v{registered_model.version}"
            )

            # è¨­ç½®æ¨¡å‹åˆ° Staging éšæ®µ
            client = mlflow.tracking.MlflowClient()
            client.transition_model_version_stage(
                name=model_name, version=registered_model.version, stage="Staging"
            )
            logger.info(
                f"âœ… å·²å°‡æ¨¡å‹è¨­ç½®ç‚º Staging éšæ®µ: {model_name} v{registered_model.version}"
            )

            # å‰µå»ºæ¨¡å‹å¡ç‰‡
            model_id = f"task_{run.info.run_name}"  # ä½¿ç”¨ run name ä½œç‚ºå”¯ä¸€æ¨™è­˜

            # å‰µå»ºæ–°çš„æ¨¡å‹å¡ç‰‡
            model_card = ModelCard(
                id=model_id,
                name=model_name,  # ä½¿ç”¨ model_name è€Œä¸æ˜¯ run_name
                base_model=config.model.name.split("/")[-1],
                language="en",
                task="text-classification",
                description="LoRA fine-tuned model on glue dataset",
                version=registered_model.version,  # ç›´æ¥è¨­ç½®ç‰ˆæœ¬
                stage="Staging",  # ç›´æ¥è¨­ç½®éšæ®µ
                run_id=run.info.run_id,
                tags=["glue", "lora", config.model.name.split("/")[-1]],
            )

            # ä¿å­˜æ¨¡å‹å¡ç‰‡
            registry.save_model_card(model_card)
            logger.info(f"âœ… æ¨¡å‹å¡ç‰‡å·²å‰µå»º: {model_id}")

            # Log training logs if exists
            log_file = os.path.join(str(trainer.args.output_dir), "logs.txt")
            if os.path.exists(log_file):
                mlflow.log_artifact(log_file, "logs")
            else:
                logger.warning("âš ï¸ æ‰¾ä¸åˆ°è¨“ç·´æ—¥èªŒæ–‡ä»¶")

            # Log config file if exists
            config_path = os.path.join(str(trainer.args.output_dir), "config.yaml")
            if os.path.exists(config_path):
                mlflow.log_artifact(config_path, "config")
            else:
                logger.warning("âš ï¸ æ‰¾ä¸åˆ°é…ç½®æ–‡ä»¶")

            # è¨“ç·´ç¸½çµ
            logger.info("ğŸ¯ è¨“ç·´ç¸½çµ:")
            logger.info(f"   - ç¸½è¨“ç·´æ­¥æ•¸: {train_result.global_step}")
            logger.info(
                f"   - ç¸½è¨“ç·´æ™‚é–“: {train_result.metrics['train_runtime']:.2f} ç§’"
            )
            logger.info(f"   - é©—è­‰æº–ç¢ºç‡: {eval_result['eval_accuracy']:.4f}")
            logger.info(f"   - æ¨¡å‹ä¿å­˜ä½ç½®: {output_dir}")

            return train_result, eval_result, run.info.run_id

        except RuntimeError as e:
            if "out of memory" in str(e):
                if torch.cuda.is_available():
                    current_gpu_memory = torch.cuda.memory_allocated() / 1024**3
                    total_gpu_memory = (
                        torch.cuda.get_device_properties(0).total_memory / 1024**3
                    )
                    raise RuntimeError(
                        f"GPU è¨˜æ†¶é«”ä¸è¶³: å·²ä½¿ç”¨ {current_gpu_memory:.1f}GB / ç¸½è¨ˆ {total_gpu_memory:.1f}GB"
                    ) from e
                else:
                    current_memory = psutil.Process().memory_info().rss / 1024**3
                    total_memory = psutil.virtual_memory().total / 1024**3
                    raise RuntimeError(
                        f"CPU è¨˜æ†¶é«”ä¸è¶³: å·²ä½¿ç”¨ {current_memory:.1f}GB / ç¸½è¨ˆ {total_memory:.1f}GB"
                    ) from e
            raise
