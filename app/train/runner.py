"""
è¨“ç·´ä¸»æµç¨‹ç›¸é—œåŠŸèƒ½
"""

import logging
from pathlib import Path
from typing import Dict, Optional, Tuple, Union

import psutil
import torch
from datasets import Dataset
from peft import LoraConfig, get_peft_model
from transformers import (
    AutoModelForSequenceClassification,
    AutoTokenizer,
    PreTrainedModel,
    PreTrainedTokenizer,
    Trainer,
    TrainingArguments,
)

from app.core.config import Config
from app.train.evaluator import TrainingProgressCallback, compute_metrics

logger = logging.getLogger(__name__)


def setup_device(config: Config) -> torch.device:
    """è¨­ç½®è¨“ç·´è¨­å‚™

    Args:
        config: è¨“ç·´é…ç½®

    Returns:
        torch.device: è¨“ç·´è¨­å‚™
    """
    if config.training.device:
        return torch.device(config.training.device)

    if torch.cuda.is_available():
        device = torch.device("cuda")
        logger.info("ğŸš€ ä½¿ç”¨ CUDA GPU åŠ é€Ÿ")
    elif torch.backends.mps.is_available():
        device = torch.device("mps")
        logger.info("ğŸš€ ä½¿ç”¨ MPS åŠ é€Ÿï¼ˆApple Siliconï¼‰")
    else:
        device = torch.device("cpu")
        logger.info("âš ï¸ æœªæª¢æ¸¬åˆ° GPUï¼Œä½¿ç”¨ CPU æ¨¡å¼")

    return device


def load_model_and_tokenizer(
    config: Config, device: torch.device
) -> Tuple[PreTrainedModel, PreTrainedTokenizer]:
    """è¼‰å…¥æ¨¡å‹èˆ‡ tokenizer

    Args:
        config: è¨“ç·´é…ç½®
        device: è¨“ç·´è¨­å‚™

    Returns:
        tuple: (model, tokenizer)
    """
    logger.info("ğŸ“¥ è¼‰å…¥æ¨¡å‹èˆ‡ tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(config.model.name)
    model = AutoModelForSequenceClassification.from_pretrained(
        config.model.name, num_labels=config.model.num_labels
    ).to(device)
    logger.info(f"âœ… æ¨¡å‹è¼‰å…¥å®Œæˆ: {config.model.name}")
    return model, tokenizer


def setup_lora(
    config: Config, model: PreTrainedModel, device: torch.device
) -> PreTrainedModel:
    """è¨­ç½® LoRA

    Args:
        config: è¨“ç·´é…ç½®
        model: åŸºç¤æ¨¡å‹
        device: è¨“ç·´è¨­å‚™

    Returns:
        PreTrainedModel: åŠ å…¥ LoRA å¾Œçš„æ¨¡å‹
    """
    lora_config = LoraConfig(
        r=config.lora.r,
        lora_alpha=config.lora.lora_alpha,
        target_modules=config.lora.target_modules,
        lora_dropout=config.lora.lora_dropout,
        bias=config.lora.bias,
        task_type=config.lora.task_type,
    )
    model = get_peft_model(model, lora_config).to(device)
    logger.info("âœ… LoRA é…ç½®å®Œæˆ")

    # é¡¯ç¤ºå¯è¨“ç·´åƒæ•¸æ•¸é‡
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total_params = sum(p.numel() for p in model.parameters())
    logger.info(
        f"ğŸ“Š å¯è¨“ç·´åƒæ•¸: {trainable_params:,} / {total_params:,} ({trainable_params / total_params * 100:.2f}%)"
    )

    return model


def setup_training(
    config: Config,
    model: PreTrainedModel,
    train_dataset: Dataset,
    eval_dataset: Dataset,
    exp_dir: Path,
) -> Trainer:
    """è¨­ç½®è¨“ç·´

    Args:
        config: è¨“ç·´é…ç½®
        model: æ¨¡å‹
        train_dataset: è¨“ç·´è³‡æ–™é›†
        eval_dataset: é©—è­‰è³‡æ–™é›†
        exp_dir: å¯¦é©—ç›®éŒ„

    Returns:
        Trainer: è¨“ç·´å™¨å¯¦ä¾‹
    """
    # è¨“ç·´åƒæ•¸
    logger.info("âš™ï¸ è¨­ç½®è¨“ç·´åƒæ•¸...")
    training_args = TrainingArguments(
        output_dir=config.training.output_dir,
        learning_rate=config.training.learning_rate,
        per_device_train_batch_size=config.training.per_device_train_batch_size,
        num_train_epochs=config.training.num_train_epochs,
        logging_steps=config.training.logging_steps,
        report_to=None,
        # Checkpoint ç›¸é—œé…ç½®
        save_strategy="epoch",  # æ¯å€‹ epoch ä¿å­˜ä¸€æ¬¡
        save_total_limit=None,  # ä¸é™åˆ¶ä¿å­˜æ•¸é‡ï¼Œç”± CheckpointManager ç®¡ç†
        eval_strategy="epoch",  # æ¯å€‹ epoch è©•ä¼°ä¸€æ¬¡
        load_best_model_at_end=True,  # è¨“ç·´çµæŸå¾Œè¼‰å…¥æœ€ä½³æ¨¡å‹
        metric_for_best_model="eval_accuracy",  # ä½¿ç”¨é©—è­‰æº–ç¢ºç‡é¸æ“‡æœ€ä½³æ¨¡å‹
        greater_is_better=True,  # æŒ‡æ¨™è¶Šå¤§è¶Šå¥½
    )

    logger.info("ğŸ“ è¨“ç·´åƒæ•¸:")
    logger.info(f"   - å­¸ç¿’ç‡: {training_args.learning_rate}")
    logger.info(f"   - æ‰¹æ¬¡å¤§å°: {training_args.per_device_train_batch_size}")
    logger.info(f"   - è¨“ç·´è¼ªæ•¸: {training_args.num_train_epochs}")
    logger.info(f"   - è¨˜éŒ„é »ç‡: æ¯ {training_args.logging_steps} æ­¥")

    logger.info("ğŸ“ Checkpoint è¨­ç½®:")
    logger.info("   - ä¿å­˜ç­–ç•¥: æ¯å€‹ epoch")
    logger.info("   - è©•ä¼°ç­–ç•¥: æ¯å€‹ epoch")
    logger.info(f"   - è¼‰å…¥æœ€ä½³æ¨¡å‹: {training_args.load_best_model_at_end}")
    logger.info(f"   - è©•ä¼°æŒ‡æ¨™: {training_args.metric_for_best_model}")
    logger.info("   - ä¿ç•™ä¸‰å€‹é—œéµ checkpoints:")
    logger.info("     1. æœ€ä½³è©•ä¼°æº–ç¢ºç‡")
    logger.info("     2. æœ€å¾Œä¸€å€‹ï¼ˆç”¨æ–¼æ¢å¾©è¨“ç·´ï¼‰")
    logger.info("     3. è¨“ç·´æ™‚é–“æœ€çŸ­ï¼ˆç”¨æ–¼å¿«é€Ÿå¯¦é©—ï¼‰")

    # å‰µå»ºè‡ªå®šç¾© callbackï¼Œä½¿ç”¨å¯¦é©—ç›®éŒ„ä¸­çš„æ—¥èªŒæ–‡ä»¶
    progress_callback = TrainingProgressCallback(exp_dir / "logs.txt")

    # å‰µå»º Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        compute_metrics=compute_metrics,
        callbacks=[progress_callback],
    )

    return trainer


def train_and_evaluate(config: Config, trainer: Trainer) -> Tuple[Dict, Optional[Dict]]:
    """è¨“ç·´èˆ‡è©•ä¼°

    Args:
        config: è¨“ç·´é…ç½®
        trainer: è¨“ç·´å™¨å¯¦ä¾‹

    Returns:
        tuple: (train_result, eval_result) è¨“ç·´çµæœå’Œè©•ä¼°çµæœ

    Raises:
        RuntimeError: ç•¶è¨˜æ†¶é«”ä¸è¶³æ™‚
    """
    logger.info("ğŸš€ é–‹å§‹è¨“ç·´...")
    logger.info("=" * 50)

    # æª¢æŸ¥åˆå§‹è¨˜æ†¶é«”ç‹€æ…‹
    if torch.cuda.is_available():
        torch.cuda.reset_peak_memory_stats()
        initial_gpu_memory = torch.cuda.memory_allocated() / 1024**3  # GB
        logger.info(f"åˆå§‹ GPU è¨˜æ†¶é«”ä½¿ç”¨: {initial_gpu_memory:.2f}GB")

    # è¨“ç·´
    try:
        train_result = trainer.train()
        logger.info("ğŸ‰ è¨“ç·´å®Œæˆï¼")

        # è¨˜éŒ„æœ€å¤§è¨˜æ†¶é«”ä½¿ç”¨é‡
        if torch.cuda.is_available():
            peak_gpu_memory = torch.cuda.max_memory_allocated() / 1024**3  # GB
            current_gpu_memory = torch.cuda.memory_allocated() / 1024**3  # GB
            logger.info(f"æœ€å¤§ GPU è¨˜æ†¶é«”ä½¿ç”¨: {peak_gpu_memory:.2f}GB")
            logger.info(f"ç•¶å‰ GPU è¨˜æ†¶é«”ä½¿ç”¨: {current_gpu_memory:.2f}GB")

            # æª¢æŸ¥æ˜¯å¦æ¥è¿‘è¨˜æ†¶é«”é™åˆ¶
            total_gpu_memory = (
                torch.cuda.get_device_properties(0).total_memory / 1024**3
            )
            if peak_gpu_memory > total_gpu_memory * 0.9:  # ä½¿ç”¨è¶…é 90% çš„è¨˜æ†¶é«”
                logger.warning(
                    f"âš ï¸ GPU è¨˜æ†¶é«”ä½¿ç”¨ç‡éé«˜: {(peak_gpu_memory / total_gpu_memory) * 100:.1f}%"
                )
    except RuntimeError as e:
        if "out of memory" in str(e):
            if torch.cuda.is_available():
                current_gpu_memory = torch.cuda.memory_allocated() / 1024**3
                total_gpu_memory = (
                    torch.cuda.get_device_properties(0).total_memory / 1024**3
                )
                raise RuntimeError(
                    f"GPU è¨˜æ†¶é«”ä¸è¶³: å·²ä½¿ç”¨ {current_gpu_memory:.1f}GB / ç¸½è¨ˆ {total_gpu_memory:.1f}GB"
                ) from e
            else:
                current_memory = psutil.Process().memory_info().rss / 1024**3
                total_memory = psutil.virtual_memory().total / 1024**3
                raise RuntimeError(
                    f"CPU è¨˜æ†¶é«”ä¸è¶³: å·²ä½¿ç”¨ {current_memory:.1f}GB / ç¸½è¨ˆ {total_memory:.1f}GB"
                ) from e
        raise

    logger.info("=" * 50)

    # è©•ä¼°
    logger.info("ğŸ“Š è©•ä¼°æ¨¡å‹...")
    eval_result = trainer.evaluate()
    logger.info(f"âœ… é©—è­‰æº–ç¢ºç‡: {eval_result['eval_accuracy']:.4f}")

    # ä¿å­˜æ¨¡å‹
    output_dir = Path(config.training.output_dir) / "final_model"
    logger.info(f"ğŸ’¾ ä¿å­˜æ¨¡å‹åˆ° {output_dir}...")
    trainer.save_model(str(output_dir))
    logger.info("âœ… æ¨¡å‹ä¿å­˜å®Œæˆ")

    # è¨“ç·´ç¸½çµ
    logger.info("ğŸ¯ è¨“ç·´ç¸½çµ:")
    logger.info(f"   - ç¸½è¨“ç·´æ­¥æ•¸: {train_result.global_step}")
    logger.info(f"   - ç¸½è¨“ç·´æ™‚é–“: {train_result.metrics['train_runtime']:.2f} ç§’")
    logger.info(f"   - é©—è­‰æº–ç¢ºç‡: {eval_result['eval_accuracy']:.4f}")
    logger.info(f"   - æ¨¡å‹ä¿å­˜ä½ç½®: {output_dir}")

    return train_result, eval_result
