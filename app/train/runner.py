"""
è¨“ç·´ä¸»æµç¨‹ç›¸é—œåŠŸèƒ½
"""

import logging
import os
import time
from datetime import datetime
from typing import Dict, Optional, Tuple

import mlflow
import psutil
import torch
from datasets import Dataset
from peft import LoraConfig, get_peft_model
from transformers import (
    AutoModelForSequenceClassification,
    AutoTokenizer,
    PreTrainedModel,
    PreTrainedTokenizer,
    Trainer,
    TrainingArguments,
)

from app.core.config import Config
from app.core.mlflow_config import init_mlflow
from app.train.evaluator import TrainingProgressCallback, compute_metrics

logger = logging.getLogger(__name__)


def load_model_and_tokenizer(
    config: Config, device: str
) -> Tuple[PreTrainedModel, PreTrainedTokenizer]:
    """è¼‰å…¥æ¨¡å‹å’Œåˆ†è©å™¨

    Args:
        config: è¨“ç·´é…ç½®
        device: è¨­å‚™åç¨±

    Returns:
        tuple: (model, tokenizer) æ¨¡å‹å’Œåˆ†è©å™¨
    """
    logger.info(f"ğŸ¤– è¼‰å…¥æ¨¡å‹ {config.model.name}...")
    model = AutoModelForSequenceClassification.from_pretrained(
        config.model.name,
        num_labels=config.model.num_labels,
        trust_remote_code=True,
    )
    tokenizer = AutoTokenizer.from_pretrained(config.model.name)

    # ç§»å‹•æ¨¡å‹åˆ°æŒ‡å®šè¨­å‚™
    model = model.to(device)
    logger.info(f"âœ… æ¨¡å‹å·²è¼‰å…¥åˆ° {device}")

    return model, tokenizer


def setup_lora(config: Config, model: PreTrainedModel, device: str) -> PreTrainedModel:
    """è¨­ç½® LoRA é…ç½®

    Args:
        config: è¨“ç·´é…ç½®
        model: åŸºç¤æ¨¡å‹
        device: è¨­å‚™åç¨±

    Returns:
        PreTrainedModel: æ·»åŠ  LoRA å¾Œçš„æ¨¡å‹
    """
    logger.info("ğŸ”§ è¨­ç½® LoRA é…ç½®...")
    lora_config = LoraConfig(
        r=config.lora.r,
        lora_alpha=config.lora.lora_alpha,
        target_modules=config.lora.target_modules,
        lora_dropout=config.lora.lora_dropout,
        bias="none",
        task_type="SEQ_CLS",
    )

    # æ·»åŠ  LoRA é©é…å™¨
    model = get_peft_model(model, lora_config)
    model = model.to(device)

    # æ‰“å°åƒæ•¸çµ±è¨ˆ
    model.print_trainable_parameters()
    logger.info("âœ… LoRA é…ç½®å®Œæˆ")

    return model


def setup_device(config: Config) -> str:
    """è¨­ç½®è¨“ç·´è¨­å‚™

    Args:
        config: è¨“ç·´é…ç½®

    Returns:
        str: è¨­å‚™åç¨±
    """
    if config.training.device == "auto":
        if torch.cuda.is_available():
            device = "cuda"
        elif torch.backends.mps.is_available():
            device = "mps"
        else:
            device = "cpu"
    else:
        device = config.training.device

    logger.info(f"ğŸ’» ä½¿ç”¨è¨­å‚™: {device}")
    return device


def load_and_process_data(
    config: Config, tokenizer: PreTrainedTokenizer
) -> Tuple[Dataset, Dataset]:
    """è¼‰å…¥ä¸¦è™•ç†æ•¸æ“š

    Args:
        config: è¨“ç·´é…ç½®
        tokenizer: åˆ†è©å™¨

    Returns:
        tuple: (train_dataset, eval_dataset) è¨“ç·´é›†å’Œé©—è­‰é›†
    """
    logger.info("ğŸ“¦ è¼‰å…¥æ•¸æ“šé›†...")

    # è¼‰å…¥æ•¸æ“šé›†
    from datasets import load_dataset

    dataset = load_dataset(
        config.data.dataset_name,
        config.data.subset,
        split="train",
        trust_remote_code=True,
    )

    # éš¨æ©Ÿåˆ†å‰²è¨“ç·´é›†å’Œé©—è­‰é›†
    dataset = dataset.train_test_split(test_size=0.1, seed=42)
    train_dataset = dataset["train"]
    eval_dataset = dataset["test"]

    # å¦‚æœæŒ‡å®šäº†è¨“ç·´æ¨£æœ¬æ•¸ï¼Œå‰‡åªä½¿ç”¨éƒ¨åˆ†æ•¸æ“š
    if config.training.train_samples:
        train_dataset = train_dataset.select(range(config.training.train_samples))

    logger.info(f"âœ… è¨“ç·´é›†å¤§å°: {len(train_dataset)}")
    logger.info(f"âœ… é©—è­‰é›†å¤§å°: {len(eval_dataset)}")

    return train_dataset, eval_dataset


def setup_training(
    config: Config,
    model: PreTrainedModel,
    train_dataset: Dataset,
    eval_dataset: Dataset,
    exp_dir: str,
) -> Trainer:
    """è¨­ç½®è¨“ç·´

    Args:
        config: è¨“ç·´é…ç½®
        model: æ¨¡å‹
        train_dataset: è¨“ç·´è³‡æ–™é›†
        eval_dataset: é©—è­‰è³‡æ–™é›†
        exp_dir: å¯¦é©—ç›®éŒ„

    Returns:
        Trainer: è¨“ç·´å™¨å¯¦ä¾‹
    """
    # ç¢ºä¿ exp_dir æ˜¯å­—ç¬¦ä¸²
    exp_dir = str(exp_dir)

    # è¨“ç·´åƒæ•¸
    logger.info("âš™ï¸ è¨­ç½®è¨“ç·´åƒæ•¸...")
    training_args = TrainingArguments(
        output_dir=config.training.output_dir,
        learning_rate=config.training.learning_rate,
        per_device_train_batch_size=config.training.per_device_train_batch_size,
        num_train_epochs=config.training.num_train_epochs,
        logging_steps=config.training.logging_steps,
        report_to=None,
        # Checkpoint ç›¸é—œé…ç½®
        save_strategy="epoch",  # æ¯å€‹ epoch ä¿å­˜ä¸€æ¬¡
        save_total_limit=None,  # ä¸é™åˆ¶ä¿å­˜æ•¸é‡ï¼Œç”± CheckpointManager ç®¡ç†
        eval_strategy="epoch",  # æ¯å€‹ epoch è©•ä¼°ä¸€æ¬¡
        load_best_model_at_end=True,  # è¨“ç·´çµæŸå¾Œè¼‰å…¥æœ€ä½³æ¨¡å‹
        metric_for_best_model="eval_accuracy",  # ä½¿ç”¨é©—è­‰æº–ç¢ºç‡é¸æ“‡æœ€ä½³æ¨¡å‹
        greater_is_better=True,  # æŒ‡æ¨™è¶Šå¤§è¶Šå¥½
    )

    logger.info("ğŸ“ è¨“ç·´åƒæ•¸:")
    logger.info(f"   - å­¸ç¿’ç‡: {training_args.learning_rate}")
    logger.info(f"   - æ‰¹æ¬¡å¤§å°: {training_args.per_device_train_batch_size}")
    logger.info(f"   - è¨“ç·´è¼ªæ•¸: {training_args.num_train_epochs}")
    logger.info(f"   - è¨˜éŒ„é »ç‡: æ¯ {training_args.logging_steps} æ­¥")

    logger.info("ğŸ“ Checkpoint è¨­ç½®:")
    logger.info("   - ä¿å­˜ç­–ç•¥: æ¯å€‹ epoch")
    logger.info("   - è©•ä¼°ç­–ç•¥: æ¯å€‹ epoch")
    logger.info(f"   - è¼‰å…¥æœ€ä½³æ¨¡å‹: {training_args.load_best_model_at_end}")
    logger.info(f"   - è©•ä¼°æŒ‡æ¨™: {training_args.metric_for_best_model}")
    logger.info("   - ä¿ç•™ä¸‰å€‹é—œéµ checkpoints:")
    logger.info("     1. æœ€ä½³è©•ä¼°æº–ç¢ºç‡")
    logger.info("     2. æœ€å¾Œä¸€å€‹ï¼ˆç”¨æ–¼æ¢å¾©è¨“ç·´ï¼‰")
    logger.info("     3. è¨“ç·´æ™‚é–“æœ€çŸ­ï¼ˆç”¨æ–¼å¿«é€Ÿå¯¦é©—ï¼‰")

    # å‰µå»ºè‡ªå®šç¾© callbackï¼Œä½¿ç”¨ artifacts ç›®éŒ„ä¸­çš„æ—¥èªŒæ–‡ä»¶
    log_file = os.path.join(str(config.training.output_dir), "logs.txt")
    # ç¢ºä¿æ—¥èªŒç›®éŒ„å­˜åœ¨
    os.makedirs(os.path.dirname(log_file), exist_ok=True)
    progress_callback = TrainingProgressCallback(log_file)

    # å‰µå»º Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        compute_metrics=compute_metrics,
        callbacks=[progress_callback],
    )

    return trainer


def train_and_evaluate(
    config: Config, trainer: Trainer
) -> Tuple[Dict, Optional[Dict], str]:
    """è¨“ç·´èˆ‡è©•ä¼°

    Args:
        config: è¨“ç·´é…ç½®
        trainer: è¨“ç·´å™¨å¯¦ä¾‹

    Returns:
        tuple: (train_result, eval_result, run_id) è¨“ç·´çµæœã€è©•ä¼°çµæœå’Œ MLflow run ID

    Raises:
        RuntimeError: ç•¶è¨˜æ†¶é«”ä¸è¶³æ™‚
    """
    # Initialize MLflow and start run
    mlflow_config = init_mlflow()

    with mlflow.start_run(
        experiment_id=mlflow_config["experiment_id"],
        run_name=f"{config.model.name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
    ) as run:
        logger.info("ğŸš€ é–‹å§‹è¨“ç·´...")
        logger.info("=" * 50)
        logger.info(f"MLflow å¯¦é©— ID: {mlflow_config['experiment_id']}")
        logger.info(f"MLflow Run ID: {run.info.run_id}")

        # Log parameters
        mlflow.log_params(
            {
                "model_name": config.model.name,
                "batch_size": config.training.per_device_train_batch_size,
                "learning_rate": config.training.learning_rate,
                "epochs": config.training.num_train_epochs,
                "device": str(trainer.args.device),
                "lora_r": config.lora.r,
                "lora_alpha": config.lora.lora_alpha,
                "lora_dropout": config.lora.lora_dropout,
            }
        )

        # æª¢æŸ¥åˆå§‹è¨˜æ†¶é«”ç‹€æ…‹
        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()
            initial_gpu_memory = torch.cuda.memory_allocated() / 1024**3  # GB
            logger.info(f"åˆå§‹ GPU è¨˜æ†¶é«”ä½¿ç”¨: {initial_gpu_memory:.2f}GB")

        try:
            # è¨“ç·´
            start_time = time.time()
            train_result = trainer.train()
            training_time = time.time() - start_time

            logger.info("ğŸ‰ è¨“ç·´å®Œæˆï¼")

            # Log training metrics
            mlflow.log_metrics(
                {
                    "training_time": training_time,
                    "total_steps": train_result.global_step,
                    "train_loss": train_result.metrics.get("train_loss", 0.0),
                    "train_runtime": train_result.metrics["train_runtime"],
                    "train_samples_per_second": train_result.metrics[
                        "train_samples_per_second"
                    ],
                }
            )

            # è¨˜éŒ„æœ€å¤§è¨˜æ†¶é«”ä½¿ç”¨é‡
            if torch.cuda.is_available():
                peak_gpu_memory = torch.cuda.max_memory_allocated() / 1024**3  # GB
                current_gpu_memory = torch.cuda.memory_allocated() / 1024**3  # GB
                logger.info(f"æœ€å¤§ GPU è¨˜æ†¶é«”ä½¿ç”¨: {peak_gpu_memory:.2f}GB")
                logger.info(f"ç•¶å‰ GPU è¨˜æ†¶é«”ä½¿ç”¨: {current_gpu_memory:.2f}GB")

                # æª¢æŸ¥æ˜¯å¦æ¥è¿‘è¨˜æ†¶é«”é™åˆ¶
                total_gpu_memory = (
                    torch.cuda.get_device_properties(0).total_memory / 1024**3
                )
                if peak_gpu_memory > total_gpu_memory * 0.9:  # ä½¿ç”¨è¶…é 90% çš„è¨˜æ†¶é«”
                    logger.warning(
                        f"âš ï¸ GPU è¨˜æ†¶é«”ä½¿ç”¨ç‡éé«˜: {(peak_gpu_memory / total_gpu_memory) * 100:.1f}%"
                    )

            logger.info("=" * 50)

            # è©•ä¼°
            logger.info("ğŸ“Š è©•ä¼°æ¨¡å‹...")
            eval_result = trainer.evaluate()
            logger.info(f"âœ… é©—è­‰æº–ç¢ºç‡: {eval_result['eval_accuracy']:.4f}")

            # Log evaluation metrics
            mlflow.log_metrics(
                {
                    "eval_accuracy": eval_result["eval_accuracy"],
                    "eval_loss": eval_result["eval_loss"],
                }
            )

            # ä¿å­˜æ¨¡å‹
            output_dir = os.path.join(config.training.output_dir, "final_model")
            os.makedirs(output_dir, exist_ok=True)
            logger.info(f"ğŸ’¾ ä¿å­˜æ¨¡å‹åˆ° {output_dir}...")
            trainer.save_model(output_dir)
            logger.info("âœ… æ¨¡å‹ä¿å­˜å®Œæˆ")

            # Log artifacts to MLflow
            mlflow.log_artifacts(output_dir, "final_model")

            # Log training logs if exists
            log_file = os.path.join(str(trainer.args.output_dir), "logs.txt")
            if os.path.exists(log_file):
                mlflow.log_artifact(log_file, "logs")
            else:
                logger.warning("âš ï¸ æ‰¾ä¸åˆ°è¨“ç·´æ—¥èªŒæ–‡ä»¶")

            # Log config file if exists
            config_path = os.path.join(str(trainer.args.output_dir), "config.yaml")
            if os.path.exists(config_path):
                mlflow.log_artifact(config_path, "config")
            else:
                logger.warning("âš ï¸ æ‰¾ä¸åˆ°é…ç½®æ–‡ä»¶")

            # è¨“ç·´ç¸½çµ
            logger.info("ğŸ¯ è¨“ç·´ç¸½çµ:")
            logger.info(f"   - ç¸½è¨“ç·´æ­¥æ•¸: {train_result.global_step}")
            logger.info(
                f"   - ç¸½è¨“ç·´æ™‚é–“: {train_result.metrics['train_runtime']:.2f} ç§’"
            )
            logger.info(f"   - é©—è­‰æº–ç¢ºç‡: {eval_result['eval_accuracy']:.4f}")
            logger.info(f"   - æ¨¡å‹ä¿å­˜ä½ç½®: {output_dir}")

            return train_result, eval_result, run.info.run_id

        except RuntimeError as e:
            if "out of memory" in str(e):
                if torch.cuda.is_available():
                    current_gpu_memory = torch.cuda.memory_allocated() / 1024**3
                    total_gpu_memory = (
                        torch.cuda.get_device_properties(0).total_memory / 1024**3
                    )
                    raise RuntimeError(
                        f"GPU è¨˜æ†¶é«”ä¸è¶³: å·²ä½¿ç”¨ {current_gpu_memory:.1f}GB / ç¸½è¨ˆ {total_gpu_memory:.1f}GB"
                    ) from e
                else:
                    current_memory = psutil.Process().memory_info().rss / 1024**3
                    total_memory = psutil.virtual_memory().total / 1024**3
                    raise RuntimeError(
                        f"CPU è¨˜æ†¶é«”ä¸è¶³: å·²ä½¿ç”¨ {current_memory:.1f}GB / ç¸½è¨ˆ {total_memory:.1f}GB"
                    ) from e
            raise
