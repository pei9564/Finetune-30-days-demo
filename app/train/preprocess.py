"""
è³‡æ–™è™•ç†ç›¸é—œåŠŸèƒ½
"""

import logging
from typing import Dict, Tuple

from datasets import Dataset, load_dataset
from transformers import PreTrainedTokenizer

from app.core.config import Config
from app.data import (
    DataValidator,
    DataVersionManager,
    analyze_distribution,
    get_data_summary,
)

logger = logging.getLogger(__name__)


def load_and_process_data(
    config: Config, tokenizer: PreTrainedTokenizer
) -> Tuple[Dataset, Dataset]:
    """è¼‰å…¥èˆ‡è™•ç†è³‡æ–™

    Args:
        config: è¨“ç·´é…ç½®
        tokenizer: tokenizer å¯¦ä¾‹

    Returns:
        tuple: (è¨“ç·´è³‡æ–™é›†, é©—è­‰è³‡æ–™é›†)

    Raises:
        ValueError: ç•¶è³‡æ–™è¼‰å…¥æˆ–è™•ç†å¤±æ•—æ™‚
    """
    logger.info("ğŸ“Š è¼‰å…¥è³‡æ–™é›†...")
    try:
        dataset = load_dataset(config.data.dataset_name, config.data.dataset_config)
    except Exception as e:
        raise ValueError(f"ç„¡æ³•è¼‰å…¥æ•¸æ“šé›† {config.data.dataset_name}: {str(e)}")

    # æª¢æŸ¥æ•¸æ“šé›†æ˜¯å¦å­˜åœ¨å¿…è¦çš„åˆ†å‰²
    required_splits = ["train", "validation"]
    for split in required_splits:
        if split not in dataset:
            raise ValueError(f"æ•¸æ“šé›†ç¼ºå°‘å¿…è¦çš„åˆ†å‰²: {split}")

    # é¸æ“‡æŒ‡å®šæ•¸é‡çš„æ¨£æœ¬
    try:
        train_small = dataset["train"].select(range(config.data.train_samples))
        eval_small = dataset["validation"].select(range(config.data.eval_samples))
    except Exception as e:
        raise ValueError(f"é¸æ“‡æ•¸æ“šæ¨£æœ¬æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")

    # æª¢æŸ¥æ•¸æ“šé›†å¤§å°
    if len(train_small) == 0:
        raise ValueError("è¨“ç·´æ•¸æ“šé›†ä¸èƒ½ç‚ºç©º")
    if len(eval_small) == 0:
        raise ValueError("é©—è­‰æ•¸æ“šé›†ä¸èƒ½ç‚ºç©º")

    logger.info(f"   - è¨“ç·´è³‡æ–™: {len(train_small)} ç­†")
    logger.info(f"   - é©—è­‰è³‡æ–™: {len(eval_small)} ç­†")

    # è³‡æ–™åˆ†æ
    logger.info("ğŸ“‹ é€²è¡Œè³‡æ–™åˆ†æèˆ‡ç®¡ç†...")
    summary = get_data_summary(train_small)
    logger.info("ğŸ“Š è³‡æ–™æ‘˜è¦:")
    logger.info(f"   - ç‰¹å¾µæ•¸: {summary['num_features']}")
    logger.info(f"   - ç‰¹å¾µåç¨±: {summary['feature_names']}")

    distribution_analysis = analyze_distribution(train_small)
    logger.info("ğŸ“Š é¡åˆ¥åˆ†å¸ƒ:")
    logger.info(f"   - é¡åˆ¥æ•¸: {distribution_analysis['num_classes']}")
    logger.info(f"   - å„é¡åˆ¥æ•¸é‡: {distribution_analysis['label_counts']}")
    logger.info(f"   - ä¸å¹³è¡¡æ¯”ä¾‹: {distribution_analysis['imbalance_ratio']:.2f}:1")
    logger.info(
        f"   - æ˜¯å¦å¹³è¡¡: {'âœ…' if distribution_analysis['is_balanced'] else 'âŒ'}"
    )

    # è³‡æ–™é©—è­‰
    validator = DataValidator(logger)
    validator.set_validation_rules(config.data.validation_rules)
    validation_report = validator.validate_dataset(train_small, ["sentence"])
    total_issues = sum(
        len(issue_list) for issue_list in validation_report["issues"].values()
    )

    if total_issues > 0:
        logger.warning(f"âš ï¸ ç™¼ç¾ {total_issues} å€‹è³‡æ–™å•é¡Œ")
        train_small = validator.clean_dataset(
            train_small, ["sentence"], validation_report
        )
        logger.info(f"ğŸ§¹ è³‡æ–™æ¸…ç†å®Œæˆï¼Œå‰©é¤˜ {len(train_small)} ç­†è¨“ç·´è³‡æ–™")
    else:
        logger.info("âœ… è³‡æ–™é©—è­‰é€šéï¼Œç„¡å•é¡Œç™¼ç¾")

    # ç‰ˆæœ¬ç®¡ç†
    try:
        version_manager = DataVersionManager(logger=logger)
        current_version = version_manager.get_current_version()
        if current_version:
            logger.info(f"ğŸ“¦ ç•¶å‰è³‡æ–™ç‰ˆæœ¬: {current_version}")
        else:
            version_name = f"sst2_train_{len(train_small)}samples"
            version_manager.create_version(
                train_small,
                version_name,
                description=f"SST-2 è¨“ç·´é›†ï¼Œç¶“éæ¸…ç†ï¼Œ{len(train_small)} ç­†è³‡æ–™",
                cleaning_strategy="ç§»é™¤ç©ºå€¼ã€HTMLæ¨™ç±¤æ¸…ç†ã€é‡è¤‡è³‡æ–™ç§»é™¤",
                source_info={
                    "dataset": f"{config.data.dataset_name}/{config.data.dataset_config}",
                    "split": "train",
                    "original_samples": config.data.train_samples,
                    "cleaned_samples": len(train_small),
                },
            )
            logger.info(f"ğŸ“¦ å‰µå»ºè³‡æ–™ç‰ˆæœ¬: {version_name}")
    except Exception as e:
        logger.warning(f"âš ï¸ ç‰ˆæœ¬ç®¡ç†å¤±æ•—: {e}")

    logger.info("=" * 50)

    # è³‡æ–™è™•ç†
    def tokenize(batch: Dict) -> Dict:
        # è¨ˆç®— token é•·åº¦
        token_lengths = [len(tokenizer.encode(text)) for text in batch["sentence"]]
        max_token_length = max(token_lengths)

        # å¦‚æœæœ‰è¶…é•·åºåˆ—ï¼Œè¨˜éŒ„è­¦å‘Š
        if max_token_length > config.data.max_length:
            num_truncated = sum(
                1 for length in token_lengths if length > config.data.max_length
            )
            logger.warning(
                f"ç™¼ç¾ {num_truncated} å€‹è¶…é•·åºåˆ— "
                f"(æœ€é•·: {max_token_length} tokens, "
                f"é™åˆ¶: {config.data.max_length} tokens)"
            )

        # åŸ·è¡Œ tokenize
        return tokenizer(
            batch["sentence"],
            padding="max_length",
            truncation=True,
            max_length=config.data.max_length,
            # ä¸è¿”å› overflowing_tokensï¼Œå› ç‚ºå®ƒæœƒæ”¹è®Šåºåˆ—é•·åº¦
            return_length=True,  # è¿”å›åºåˆ—é•·åº¦ä¿¡æ¯
        )

    train_dataset = train_small.map(tokenize, batched=True)
    eval_dataset = eval_small.map(tokenize, batched=True)
    logger.info("âœ… è¨“ç·´å’Œé©—è­‰è³‡æ–™é›†è™•ç†å®Œæˆ")

    return train_dataset, eval_dataset
