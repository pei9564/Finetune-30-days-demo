"""
è³‡æ–™é©—è­‰èˆ‡æ¸…ç†å·¥å…·
æä¾›è³‡æ–™å“è³ªæª¢æŸ¥ã€æ¸…ç†ã€é©—è­‰ç­‰åŠŸèƒ½
"""

import json
import logging
import os
import re
from typing import Any, Dict, List, Optional

import numpy as np
from datasets import Dataset


class DataValidator:
    """è³‡æ–™é©—è­‰èˆ‡æ¸…ç†å·¥å…·"""

    def __init__(self, logger: Optional[logging.Logger] = None):
        self.logger = logger or logging.getLogger(__name__)
        self.validation_rules = {
            "min_text_length": 1,
            "max_text_length": 1000,
            "allow_empty": False,
            "remove_html": True,
            "remove_special_chars": False,
            "check_encoding": True,
        }
        self.issues_found = []

    def set_validation_rules(self, rules: Dict[str, Any]) -> None:
        """è¨­å®šé©—è­‰è¦å‰‡"""
        self.validation_rules.update(rules)
        self.logger.info(f"ğŸ“‹ æ›´æ–°é©—è­‰è¦å‰‡: {rules}")

    def validate_dataset(
        self, dataset: Dataset, text_columns: List[str]
    ) -> Dict[str, Any]:
        """
        é©—è­‰æ•´å€‹è³‡æ–™é›†

        Args:
            dataset: è¦é©—è­‰çš„è³‡æ–™é›†
            text_columns: æ–‡æœ¬æ¬„ä½åˆ—è¡¨

        Returns:
            é©—è­‰å ±å‘Š
        """
        self.logger.info(f"ğŸ” é–‹å§‹é©—è­‰è³‡æ–™é›†ï¼Œå…± {len(dataset)} ç­†è³‡æ–™...")
        self.issues_found = []

        validation_report = {
            "total_samples": len(dataset),
            "text_columns": text_columns,
            "validation_rules": self.validation_rules.copy(),
            "issues": {
                "empty_values": [],
                "length_violations": [],
                "encoding_issues": [],
                "html_content": [],
                "duplicate_entries": [],
            },
            "statistics": {},
            "recommendations": [],
        }

        # æª¢æŸ¥æ¯ä¸€ç­†è³‡æ–™
        for idx in range(len(dataset)):
            sample = dataset[idx]
            self._validate_sample(sample, idx, text_columns, validation_report)

        # æª¢æŸ¥é‡è¤‡è³‡æ–™
        self._check_duplicates(dataset, text_columns, validation_report)

        # ç”Ÿæˆçµ±è¨ˆè³‡è¨Š
        self._generate_statistics(dataset, text_columns, validation_report)

        # ç”Ÿæˆå»ºè­°
        self._generate_recommendations(validation_report)

        self._log_validation_summary(validation_report)

        return validation_report

    def _validate_sample(
        self,
        sample: Dict[str, Any],
        idx: int,
        text_columns: List[str],
        report: Dict[str, Any],
    ) -> None:
        """é©—è­‰å–®ä¸€æ¨£æœ¬"""

        for col in text_columns:
            if col not in sample:
                continue

            text = sample[col]

            # æª¢æŸ¥ç©ºå€¼
            if text is None or (isinstance(text, str) and text.strip() == ""):
                report["issues"]["empty_values"].append(
                    {"index": idx, "column": col, "issue": "Empty or null value"}
                )
                continue

            if not isinstance(text, str):
                text = str(text)

            # æª¢æŸ¥é•·åº¦
            text_length = len(text)
            if (
                text_length < self.validation_rules["min_text_length"]
                or text_length > self.validation_rules["max_text_length"]
            ):
                report["issues"]["length_violations"].append(
                    {
                        "index": idx,
                        "column": col,
                        "length": text_length,
                        "min_required": self.validation_rules["min_text_length"],
                        "max_allowed": self.validation_rules["max_text_length"],
                    }
                )

            # æª¢æŸ¥ HTML æ¨™ç±¤
            if self.validation_rules["remove_html"] and self._contains_html(text):
                report["issues"]["html_content"].append(
                    {
                        "index": idx,
                        "column": col,
                        "sample_text": text[:100] + "..." if len(text) > 100 else text,
                    }
                )

            # æª¢æŸ¥ç·¨ç¢¼å•é¡Œ
            if self.validation_rules["check_encoding"] and self._has_encoding_issues(
                text
            ):
                report["issues"]["encoding_issues"].append(
                    {
                        "index": idx,
                        "column": col,
                        "sample_text": text[:100] + "..." if len(text) > 100 else text,
                    }
                )

    def _check_duplicates(
        self, dataset: Dataset, text_columns: List[str], report: Dict[str, Any]
    ) -> None:
        """æª¢æŸ¥é‡è¤‡è³‡æ–™"""
        seen_texts = set()
        duplicates = []

        for idx in range(len(dataset)):
            sample = dataset[idx]

            # çµ„åˆæ‰€æœ‰æ–‡æœ¬æ¬„ä½ä½œç‚ºå”¯ä¸€è­˜åˆ¥
            combined_text = ""
            for col in text_columns:
                if col in sample and sample[col]:
                    combined_text += str(sample[col]).strip() + " "

            combined_text = combined_text.strip()

            if combined_text in seen_texts:
                duplicates.append(
                    {
                        "index": idx,
                        "text": combined_text[:100] + "..."
                        if len(combined_text) > 100
                        else combined_text,
                    }
                )
            else:
                seen_texts.add(combined_text)

        report["issues"]["duplicate_entries"] = duplicates

    def _contains_html(self, text: str) -> bool:
        """æª¢æŸ¥æ˜¯å¦åŒ…å« HTML æ¨™ç±¤"""
        html_pattern = re.compile(r"<[^>]+>")
        return bool(html_pattern.search(text))

    def _has_encoding_issues(self, text: str) -> bool:
        """æª¢æŸ¥ç·¨ç¢¼å•é¡Œ"""
        # æª¢æŸ¥å¸¸è¦‹çš„ç·¨ç¢¼å•é¡Œå­—ç¬¦
        # \ufffd æ˜¯ Unicode æ›¿æ›å­—ç¬¦ï¼ˆreplacement characterï¼‰ï¼Œé€šå¸¸è¡¨ç¤ºç·¨ç¢¼éŒ¯èª¤
        # \x00 æ˜¯ null å­—ç¬¦ï¼Œå¯èƒ½å°è‡´å•é¡Œ
        problematic_chars = ["\ufffd", "\x00"]
        return any(char in text for char in problematic_chars)

    def _generate_statistics(
        self, dataset: Dataset, text_columns: List[str], report: Dict[str, Any]
    ) -> None:
        """ç”Ÿæˆçµ±è¨ˆè³‡è¨Š"""
        stats = {}

        for col in text_columns:
            if col not in dataset.features:
                continue

            texts = [sample[col] for sample in dataset if sample.get(col)]
            text_lengths = [len(str(text)) for text in texts if text]

            if text_lengths:
                stats[col] = {
                    "count": len(text_lengths),
                    "avg_length": np.mean(text_lengths),
                    "min_length": min(text_lengths),
                    "max_length": max(text_lengths),
                    "std_length": np.std(text_lengths),
                }

        report["statistics"] = stats

    def _generate_recommendations(self, report: Dict[str, Any]) -> None:
        """ç”Ÿæˆæ¸…ç†å»ºè­°"""
        recommendations = []
        issues = report["issues"]

        if issues["empty_values"]:
            recommendations.append(
                {
                    "priority": "high",
                    "issue": f"ç™¼ç¾ {len(issues['empty_values'])} ç­†ç©ºå€¼",
                    "action": "å»ºè­°ç§»é™¤æˆ–å¡«å……ç©ºå€¼",
                }
            )

        if issues["length_violations"]:
            recommendations.append(
                {
                    "priority": "medium",
                    "issue": f"ç™¼ç¾ {len(issues['length_violations'])} ç­†é•·åº¦ç•°å¸¸",
                    "action": "å»ºè­°èª¿æ•´æ–‡æœ¬é•·åº¦é™åˆ¶æˆ–æˆªæ–·/æ“´å……æ–‡æœ¬",
                }
            )

        if issues["html_content"]:
            recommendations.append(
                {
                    "priority": "medium",
                    "issue": f"ç™¼ç¾ {len(issues['html_content'])} ç­†åŒ…å« HTML",
                    "action": "å»ºè­°æ¸…ç† HTML æ¨™ç±¤",
                }
            )

        if issues["encoding_issues"]:
            recommendations.append(
                {
                    "priority": "high",
                    "issue": f"ç™¼ç¾ {len(issues['encoding_issues'])} ç­†ç·¨ç¢¼å•é¡Œ",
                    "action": "å»ºè­°ä¿®å¾©ç·¨ç¢¼æˆ–ç§»é™¤å•é¡Œå­—ç¬¦",
                }
            )

        if issues["duplicate_entries"]:
            recommendations.append(
                {
                    "priority": "medium",
                    "issue": f"ç™¼ç¾ {len(issues['duplicate_entries'])} ç­†é‡è¤‡è³‡æ–™",
                    "action": "å»ºè­°ç§»é™¤é‡è¤‡é …ç›®",
                }
            )

        report["recommendations"] = recommendations

    def _log_validation_summary(self, report: Dict[str, Any]) -> None:
        """è¨˜éŒ„é©—è­‰æ‘˜è¦"""
        issues = report["issues"]
        total_issues = sum(len(issue_list) for issue_list in issues.values())

        self.logger.info("ğŸ” é©—è­‰å®Œæˆ!")
        self.logger.info(f"   - ç¸½æ¨£æœ¬æ•¸: {report['total_samples']}")
        self.logger.info(f"   - ç™¼ç¾å•é¡Œ: {total_issues} ç­†")

        if total_issues > 0:
            self.logger.warning("âš ï¸ ç™¼ç¾çš„å•é¡Œ:")
            for issue_type, issue_list in issues.items():
                if issue_list:
                    self.logger.warning(f"   - {issue_type}: {len(issue_list)} ç­†")
        else:
            self.logger.info("âœ… æ²’æœ‰ç™¼ç¾è³‡æ–™å•é¡Œ!")

    def clean_dataset(
        self,
        dataset: Dataset,
        text_columns: List[str],
        validation_report: Optional[Dict[str, Any]] = None,
    ) -> Dataset:
        """
        æ ¹æ“šé©—è­‰çµæœæ¸…ç†è³‡æ–™é›†

        Args:
            dataset: åŸå§‹è³‡æ–™é›†
            text_columns: æ–‡æœ¬æ¬„ä½åˆ—è¡¨
            validation_report: é©—è­‰å ±å‘Šï¼ˆå¯é¸ï¼‰

        Returns:
            æ¸…ç†å¾Œçš„è³‡æ–™é›†
        """
        self.logger.info("ğŸ§¹ é–‹å§‹æ¸…ç†è³‡æ–™é›†...")

        if validation_report is None:
            validation_report = self.validate_dataset(dataset, text_columns)

        # è¨˜éŒ„è¦ç§»é™¤çš„ç´¢å¼•
        indices_to_remove = set()

        # è™•ç†ç©ºå€¼
        for issue in validation_report["issues"]["empty_values"]:
            if not self.validation_rules["allow_empty"]:
                indices_to_remove.add(issue["index"])

        # è™•ç†ç·¨ç¢¼å•é¡Œ
        for issue in validation_report["issues"]["encoding_issues"]:
            indices_to_remove.add(issue["index"])

        # è™•ç†é‡è¤‡è³‡æ–™
        for issue in validation_report["issues"]["duplicate_entries"]:
            indices_to_remove.add(issue["index"])

        # æ¸…ç†æ–‡æœ¬å…§å®¹
        cleaned_data = []
        for idx in range(len(dataset)):
            if idx in indices_to_remove:
                continue

            sample = dict(dataset[idx])

            # æ¸…ç†æ¯å€‹æ–‡æœ¬æ¬„ä½
            for col in text_columns:
                if col in sample and sample[col]:
                    cleaned_text = self._clean_text(str(sample[col]))
                    sample[col] = cleaned_text

            cleaned_data.append(sample)

        # å‰µå»ºæ–°çš„è³‡æ–™é›†
        if cleaned_data:
            cleaned_dataset = Dataset.from_list(cleaned_data)
            self.logger.info(
                f"âœ… æ¸…ç†å®Œæˆ: {len(dataset)} â†’ {len(cleaned_dataset)} ç­†è³‡æ–™"
            )
            return cleaned_dataset
        else:
            self.logger.warning("âš ï¸ æ¸…ç†å¾Œæ²’æœ‰å‰©é¤˜è³‡æ–™!")
            return dataset

    def _clean_text(self, text: str) -> str:
        """æ¸…ç†å–®ä¸€æ–‡æœ¬"""
        # ç§»é™¤ HTML æ¨™ç±¤
        if self.validation_rules["remove_html"]:
            text = re.sub(r"<[^>]+>", "", text)

        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼ˆå¦‚æœè¨­å®šï¼‰
        if self.validation_rules["remove_special_chars"]:
            text = re.sub(r"[^\w\s]", "", text)

        # æ¸…ç†ç©ºç™½å­—ç¬¦
        text = re.sub(r"\s+", " ", text).strip()

        return text

    def save_validation_report(self, report: Dict[str, Any], save_path: str) -> None:
        """ä¿å­˜é©—è­‰å ±å‘Š"""
        os.makedirs(os.path.dirname(save_path), exist_ok=True)

        with open(save_path, "w", encoding="utf-8") as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        self.logger.info(f"ğŸ“„ é©—è­‰å ±å‘Šå·²ä¿å­˜è‡³: {save_path}")


# ç¤ºä¾‹ä½¿ç”¨å‡½æ•¸
def validate_sst2_example():
    """ç¤ºä¾‹ï¼šé©—è­‰ SST-2 è³‡æ–™é›†"""
    from datasets import load_dataset

    # è¨­å®š logger
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)

    # å‰µå»ºé©—è­‰å™¨
    validator = DataValidator(logger)

    # è¨­å®šé©—è­‰è¦å‰‡
    validator.set_validation_rules(
        {
            "min_text_length": 5,
            "max_text_length": 500,
            "allow_empty": False,
            "remove_html": True,
        }
    )

    # è¼‰å…¥è³‡æ–™é›†
    dataset = load_dataset("glue", "sst2")
    train_dataset = dataset["train"].select(range(1000))  # å– 1000 ç­†ç¯„ä¾‹

    # é©—è­‰è³‡æ–™é›†
    report = validator.validate_dataset(train_dataset, ["sentence"])

    # ä¿å­˜é©—è­‰å ±å‘Š
    os.makedirs("data/metadata", exist_ok=True)
    validator.save_validation_report(
        report, "data/metadata/sst2_validation_report.json"
    )

    # æ¸…ç†è³‡æ–™é›†
    cleaned_dataset = validator.clean_dataset(train_dataset, ["sentence"], report)

    return report, cleaned_dataset


if __name__ == "__main__":
    # åŸ·è¡Œç¤ºä¾‹é©—è­‰
    validate_sst2_example()
