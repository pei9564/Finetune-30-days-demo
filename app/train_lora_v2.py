"""
LoRA è¨“ç·´è…³æœ¬ v2
ä½¿ç”¨çµ±ä¸€é…ç½®ç³»çµ±
"""

import argparse
import logging
import os
import time
from datetime import datetime

import yaml

from app.core.config import load_config
from app.core.logger import setup_system_logger
from app.db import Database, ExperimentRecord
from app.models.model_registry import ModelCard, registry
from app.monitor import SystemMetricsMonitor, save_training_metrics
from app.tools.checkpoint_manager import CheckpointManager
from app.train import (
    load_and_process_data,
    load_model_and_tokenizer,
    setup_device,
    setup_lora,
    setup_training,
    train_and_evaluate,
)

# å…¨å±€ loggerï¼Œæœƒåœ¨ setup_experiment_dir ä¸­åˆå§‹åŒ–
logger: logging.Logger = logging.getLogger(__name__)


def setup_experiment_dir(config):
    """è¨­ç½®å¯¦é©—ç›®éŒ„ï¼Œæ·»åŠ éŒ¯èª¤è™•ç†"""
    global logger

    try:
        # ç”Ÿæˆæ™‚é–“æˆ³
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # ç¢ºä¿ results ç›®éŒ„å­˜åœ¨
        results_dir = "results"
        os.makedirs(results_dir, exist_ok=True)

        # å»ºç«‹å¯¦é©—ç›®éŒ„
        exp_dir = os.path.join("results", f"{config.experiment_name}_{timestamp}")
        os.makedirs(exp_dir, exist_ok=True)

        # å»ºç«‹å­ç›®éŒ„
        artifacts_dir = os.path.join(exp_dir, "artifacts")
        os.makedirs(artifacts_dir, exist_ok=True)

        # è¨­ç½®æ—¥èªŒæ–‡ä»¶
        log_file = os.path.join(exp_dir, "logs.txt")

        # æ›´æ–°é…ç½®ä¸­çš„è·¯å¾‘
        config.training.output_dir = artifacts_dir

        # è¨­ç½®å…¨å±€ logger
        logger = setup_system_logger(name=f"experiment_{timestamp}", log_file=log_file)

        return exp_dir

    except (OSError, PermissionError) as e:
        # å¦‚æœå‰µå»ºç›®éŒ„å¤±æ•—ï¼Œå˜—è©¦å‰µå»ºè‡¨æ™‚ç›®éŒ„
        import logging

        temp_logger = logging.getLogger(__name__)
        temp_logger.error(f"å‰µå»ºå¯¦é©—ç›®éŒ„å¤±æ•—: {e}")

        try:
            # å‰µå»ºè‡¨æ™‚ç›®éŒ„ä½œç‚ºå‚™ç”¨
            temp_dir = os.path.join("results", f"temp_{timestamp}")
            os.makedirs(temp_dir, exist_ok=True)

            # è¨­ç½®åŸºæœ¬çš„ logger
            temp_log_file = os.path.join(temp_dir, "logs.txt")
            logger = setup_system_logger(
                name=f"temp_experiment_{timestamp}", log_file=temp_log_file
            )
            logger.warning(f"ä½¿ç”¨è‡¨æ™‚ç›®éŒ„: {temp_dir}")

            return temp_dir
        except Exception as fallback_error:
            temp_logger.error(f"å‰µå»ºè‡¨æ™‚ç›®éŒ„ä¹Ÿå¤±æ•—: {fallback_error}")
            raise RuntimeError(f"ç„¡æ³•å‰µå»ºå¯¦é©—ç›®éŒ„: {e}")


def save_experiment_results(exp_dir, config, train_result, eval_result, trainer):
    """ä¿å­˜å¯¦é©—çµæœä¸¦ç®¡ç† checkpoints"""
    # æ¸…ç†ç•¶å‰å¯¦é©—çš„ checkpoints
    artifacts_dir = os.path.join(exp_dir, "artifacts")
    checkpoint_manager = CheckpointManager(results_dir=artifacts_dir)
    checkpoint_manager.cleanup_experiment(artifacts_dir)

    # å‰µå»ºç³»çµ±ç›£æ§å™¨
    monitor = SystemMetricsMonitor()

    # æ›´æ–°åºåˆ—é•·åº¦çµ±è¨ˆ
    for batch in trainer.get_train_dataloader():
        monitor.update_sequence_length(
            len(batch["input_ids"]), batch["input_ids"].shape[1]
        )

    # ä¿å­˜å®Œæ•´æŒ‡æ¨™
    metrics = save_training_metrics(
        metrics_file=os.path.join(exp_dir, "metrics.json"),
        train_result=train_result,
        eval_result=eval_result,
        system_metrics=monitor.get_system_metrics(),
        tokens_info=monitor.get_tokens_info(),
        start_time=time.time(),
    )

    # ä¿å­˜é…ç½®
    config_dict = config.model_dump()  # ä½¿ç”¨ model_dump æ›¿ä»£ dict
    config_dict["results"] = metrics
    config_file = os.path.join(exp_dir, "config.yaml")
    with open(config_file, "w", encoding="utf-8") as f:
        yaml.dump(config_dict, f, allow_unicode=True, sort_keys=False)

    # ä¿å­˜åˆ°è³‡æ–™åº«
    db = Database()
    db.save_experiment(
        ExperimentRecord(
            id=exp_dir.name,  # ä½¿ç”¨å¯¦é©—ç›®éŒ„åç¨±ä½œç‚º ID
            name=config.experiment_name,
            created_at=datetime.fromisoformat(metrics["timestamp"]),
            config_path=str(config_file),
            train_runtime=metrics["train"]["runtime"],
            eval_accuracy=metrics["eval"]["accuracy"],
            log_path=os.path.join(exp_dir, "logs.txt"),
            # æ–°å¢æ•ˆèƒ½æŒ‡æ¨™
            tokens_per_sec=metrics["train"]["tokens_per_sec"],
            cpu_percent=metrics["system"]["cpu_percent"],
            memory_gb=metrics["system"]["memory_gb"],
            # æ–°å¢è¨“ç·´åƒæ•¸
            model_name=config.model.name,
            dataset_name=config.data.dataset_name,
            train_samples=config.data.train_samples,
            batch_size=config.training.per_device_train_batch_size,
            learning_rate=config.training.learning_rate,
            num_epochs=config.training.num_train_epochs,
        )
    )

    # ä¿å­˜åˆ°æ¨¡å‹è¨»å†Šè¡¨
    model_card = ModelCard(
        id=exp_dir.name,
        name=config.experiment_name,
        base_model=config.model.name,
        language=config.data.language if hasattr(config.data, "language") else "en",
        task=config.data.task
        if hasattr(config.data, "task")
        else "text-classification",
        description=f"LoRA fine-tuned model on {config.data.dataset_name} dataset",
        metrics=dict(
            accuracy=metrics["eval"]["accuracy"], loss=metrics["eval"].get("loss")
        ),
        tags=[config.data.dataset_name, "lora", config.model.name],
    )
    registry.save_model_card(model_card)

    logger.info(f"âœ… å¯¦é©—çµæœå·²ä¿å­˜åˆ° {exp_dir} å’Œè³‡æ–™åº«")


def parse_args():
    """è§£æå‘½ä»¤åˆ—åƒæ•¸"""
    parser = argparse.ArgumentParser(description="LoRA è¨“ç·´è…³æœ¬")
    parser.add_argument(
        "--config", type=str, default="config/default.yaml", help="é…ç½®æ–‡ä»¶è·¯å¾‘"
    )
    parser.add_argument("--experiment_name", type=str, help="å¯¦é©—åç¨±")
    parser.add_argument("--learning_rate", type=float, help="å­¸ç¿’ç‡")
    parser.add_argument("--epochs", type=int, help="è¨“ç·´è¼ªæ•¸")
    parser.add_argument("--train_samples", type=int, help="è¨“ç·´æ¨£æœ¬æ•¸")
    parser.add_argument("--device", type=str, help="è¨“ç·´è¨­å‚™")
    return parser.parse_args()


def main(config=None):
    """ä¸»å‡½æ•¸

    Args:
        config: é…ç½®å°è±¡ï¼Œå¦‚æœç‚º None å‰‡å¾å‘½ä»¤åˆ—åƒæ•¸è¼‰å…¥

    Returns:
        tuple: (train_result, eval_result) è¨“ç·´çµæœå’Œè©•ä¼°çµæœ
    """
    if config is None:
        # è§£æåƒæ•¸
        args = parse_args()

        # è¼‰å…¥é…ç½®
        config = load_config(args.config)

        # æ›´æ–°é…ç½®
        if args.experiment_name:
            config.experiment_name = args.experiment_name
        if args.learning_rate:
            config.training.learning_rate = args.learning_rate
        if args.epochs:
            config.training.num_train_epochs = args.epochs
        if args.train_samples:
            config.data.train_samples = args.train_samples
        if args.device:
            config.training.device = args.device

    # è¨­ç½®å¯¦é©—ç›®éŒ„å’Œæ—¥èªŒ
    exp_dir = setup_experiment_dir(config)
    logger.info(f"ğŸ“‚ å¯¦é©—ç›®éŒ„ï¼š{exp_dir}")

    # è¨­ç½®è¨­å‚™
    device = setup_device(config)

    # è¼‰å…¥æ¨¡å‹
    model, tokenizer = load_model_and_tokenizer(config, device)

    # è¼‰å…¥è³‡æ–™
    train_dataset, eval_dataset = load_and_process_data(config, tokenizer)

    # è¨­ç½® LoRA
    model = setup_lora(config, model, device)

    # è¨­ç½®è¨“ç·´
    trainer = setup_training(config, model, train_dataset, eval_dataset, exp_dir)

    # è¨“ç·´èˆ‡è©•ä¼°
    train_result, eval_result = train_and_evaluate(config, trainer)

    # ä¿å­˜å¯¦é©—çµæœ
    save_experiment_results(exp_dir, config, train_result, eval_result, trainer)

    return train_result, eval_result


if __name__ == "__main__":
    main()
