"""
å¯¦é©—æ•ˆèƒ½åˆ†æå·¥å…·
"""

import pandas as pd

from app.db import Database, ExperimentFilter


def analyze_experiments(
    filter_params: ExperimentFilter = None,
    sort_by: str = "created_at",
    desc: bool = True,
) -> pd.DataFrame:
    """åˆ†æå¯¦é©—è¨˜éŒ„"""
    db = Database()
    records = db.list_experiments(filter_params, sort_by, desc)

    if not records:
        return pd.DataFrame()

    # è½‰æ›ç‚º DataFrame
    df = pd.DataFrame(
        [
            {
                "å¯¦é©—åç¨±": r.name,
                "æ¨¡å‹": r.model_name,
                "è³‡æ–™é›†": r.dataset_name,
                "è¨“ç·´æ¨£æœ¬æ•¸": r.train_samples,
                "æ‰¹æ¬¡å¤§å°": r.batch_size,
                "å­¸ç¿’ç‡": r.learning_rate,
                "è¨“ç·´è¼ªæ•¸": r.num_epochs,
                "è¨“ç·´æ™‚é–“": r.train_runtime,
                "è™•ç†é€Ÿåº¦": r.tokens_per_sec,
                "æº–ç¢ºç‡": r.eval_accuracy,
                "CPUä½¿ç”¨ç‡": r.cpu_percent,
                "è¨˜æ†¶é«”": r.memory_gb,
                "æ™‚é–“æˆ³": r.created_at,
            }
            for r in records
        ]
    )

    return df


def format_metrics(df: pd.DataFrame) -> pd.DataFrame:
    """æ ¼å¼åŒ–æŒ‡æ¨™é¡¯ç¤º"""
    if df.empty:
        return df

    formatted = df.copy()
    formatted["è¨“ç·´æ™‚é–“"] = formatted["è¨“ç·´æ™‚é–“"].map("{:.2f}ç§’".format)
    formatted["è™•ç†é€Ÿåº¦"] = formatted["è™•ç†é€Ÿåº¦"].map("{:.2f} tokens/s".format)
    formatted["æº–ç¢ºç‡"] = formatted["æº–ç¢ºç‡"].map("{:.2%}".format)
    formatted["CPUä½¿ç”¨ç‡"] = formatted["CPUä½¿ç”¨ç‡"].map("{:.1f}%".format)
    formatted["è¨˜æ†¶é«”"] = formatted["è¨˜æ†¶é«”"].map("{:.2f}GB".format)
    formatted["å­¸ç¿’ç‡"] = formatted["å­¸ç¿’ç‡"].map("{:.1e}".format)

    return formatted


def print_statistics(df: pd.DataFrame):
    """é¡¯ç¤ºçµ±è¨ˆè³‡è¨Š"""
    print("\nğŸ“Š å¯¦é©—æ•ˆèƒ½çµ±è¨ˆ")
    print("=" * 80)

    if df.empty:
        print("âš ï¸ æ²’æœ‰æ‰¾åˆ°ä»»ä½•å¯¦é©—è¨˜éŒ„")
        return

    db = Database()
    stats = db.get_statistics()

    print(f"ç¸½å¯¦é©—æ•¸ï¼š{stats['total_experiments']}")
    print(f"å¹³å‡è¨“ç·´æ™‚é–“ï¼š{stats['avg_runtime']:.2f} ç§’")
    print(f"å¹³å‡è™•ç†é€Ÿåº¦ï¼š{stats['avg_tokens_per_sec']:.2f} tokens/sec")
    print(f"å¹³å‡æº–ç¢ºç‡ï¼š{stats['avg_accuracy']:.2%}")
    print(f"æœ€ä½³æº–ç¢ºç‡ï¼š{stats['best_accuracy']:.2%}")
    print(f"æœ€çŸ­è¨“ç·´æ™‚é–“ï¼š{stats['min_runtime']:.2f} ç§’")
    print(f"å¹³å‡ CPU ä½¿ç”¨ç‡ï¼š{stats['avg_cpu_percent']:.1f}%")
    print(f"å¹³å‡è¨˜æ†¶é«”ä½¿ç”¨ï¼š{stats['avg_memory_gb']:.2f} GB")


def print_comparison(df: pd.DataFrame):
    """é¡¯ç¤ºå¯¦é©—æ¯”è¼ƒ"""
    if df.empty:
        return

    print("\nğŸ“‹ å¯¦é©—æ¯”è¼ƒ")
    print("=" * 80)

    # è¨­ç½®é¡¯ç¤ºé¸é …
    pd.set_option("display.max_rows", None)
    pd.set_option("display.max_columns", None)
    pd.set_option("display.width", None)

    # æ ¼å¼åŒ–ä¸¦ç§»é™¤æ™‚é–“æˆ³åˆ—
    df_display = format_metrics(df).drop(columns=["æ™‚é–“æˆ³"])

    # é¡¯ç¤ºè¡¨æ ¼
    print(df_display.to_string(index=False))


def print_group_stats(df: pd.DataFrame, group_by: str):
    """é¡¯ç¤ºåˆ†çµ„çµ±è¨ˆ"""
    if df.empty:
        return

    group_name = {"model_name": "æ¨¡å‹", "dataset_name": "è³‡æ–™é›†"}[group_by]

    print(f"\nğŸ“Š æŒ‰ {group_name} åˆ†çµ„çµ±è¨ˆ")
    print("=" * 80)

    # è¨ˆç®—åˆ†çµ„çµ±è¨ˆ
    grouped = (
        df.groupby(group_name)
        .agg(
            {
                "è¨“ç·´æ™‚é–“": "mean",
                "è™•ç†é€Ÿåº¦": "mean",
                "æº–ç¢ºç‡": "mean",
                "å¯¦é©—åç¨±": "count",
            }
        )
        .rename(columns={"å¯¦é©—åç¨±": "å¯¦é©—æ•¸"})
    )

    # æ ¼å¼åŒ–
    formatted = pd.DataFrame(
        {
            "è¨“ç·´æ™‚é–“": grouped["è¨“ç·´æ™‚é–“"].map("{:.2f}ç§’".format),
            "è™•ç†é€Ÿåº¦": grouped["è™•ç†é€Ÿåº¦"].map("{:.2f} tokens/s".format),
            "æº–ç¢ºç‡": grouped["æº–ç¢ºç‡"].map("{:.2%}".format),
            "å¯¦é©—æ•¸": grouped["å¯¦é©—æ•¸"],
        }
    )

    print(formatted.to_string())


def main():
    """ä¸»å‡½æ•¸"""
    import argparse

    parser = argparse.ArgumentParser(description="åˆ†æå¯¦é©—æ•ˆèƒ½")
    parser.add_argument(
        "--group-by", choices=["model_name", "dataset_name"], help="æŒ‰æŒ‡å®šæ¬„ä½åˆ†çµ„"
    )
    parser.add_argument("--model", help="ç¯©é¸ç‰¹å®šæ¨¡å‹")
    parser.add_argument("--dataset", help="ç¯©é¸ç‰¹å®šè³‡æ–™é›†")
    parser.add_argument("--min-accuracy", type=float, help="æœ€ä½æº–ç¢ºç‡")
    parser.add_argument(
        "--sort-by",
        default="created_at",
        choices=["created_at", "eval_accuracy", "train_runtime", "tokens_per_sec"],
        help="æ’åºä¾æ“š",
    )
    parser.add_argument("--desc", action="store_true", help="é™åºæ’åº")

    args = parser.parse_args()

    # æº–å‚™ç¯©é¸æ¢ä»¶
    filter_params = ExperimentFilter(
        model_name=args.model, dataset_name=args.dataset, min_accuracy=args.min_accuracy
    )

    # ç²å–æ•¸æ“š
    df = analyze_experiments(filter_params, args.sort_by, args.desc)

    # æ˜¯å¦éœ€è¦åˆ†çµ„
    if args.group_by:
        print_group_stats(df, args.group_by)
    else:
        # é¡¯ç¤ºçµ±è¨ˆ
        print_statistics(df)

        # é¡¯ç¤ºæ¯”è¼ƒ
        print_comparison(df)

        # æä¾›åˆ†æå»ºè­°
        print("\nğŸ’¡ åˆ†æå»ºè­°ï¼š")
        print("  1. æŒ‰æ¨¡å‹åˆ†çµ„ï¼šmake analyze-by-model")
        print("  2. æŒ‰è³‡æ–™é›†åˆ†çµ„ï¼šmake analyze-by-dataset")
        print(
            "  3. ç¯©é¸ç‰¹å®šæ¨¡å‹ï¼špython -m app.tools.analyze_metrics --model <model_name>"
        )
        print("  4. ç¯©é¸æº–ç¢ºç‡ï¼špython -m app.tools.analyze_metrics --min-accuracy 0.8")


if __name__ == "__main__":
    main()
