# Finetune-30-days â€” LoRA è¨“ç·´èˆ‡è³‡æ–™ç®¡ç†

æ­¤å°ˆæ¡ˆæä¾›ä¸€å€‹æœ€å°åŒ–çš„ **LoRA å¾®èª¿ç¯„ä¾‹**ï¼Œæ”¯æ´ M3 æ™¶ç‰‡ (MPS)ã€NVIDIA GPU (CUDA) èˆ‡ CPUã€‚
è¨­è¨ˆç›®æ¨™ï¼šå¿«é€Ÿå»ºç«‹ç’°å¢ƒã€é©—è­‰æµç¨‹ã€ä¿å­˜çµæœï¼Œä¸¦å…·å‚™ **è³‡æ–™ç‰ˆæœ¬ç®¡ç†èˆ‡é©—è­‰æ©Ÿåˆ¶**ã€‚

---

## ğŸ“‚ å°ˆæ¡ˆçµæ§‹

```
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ config.py                  # é…ç½®å®šç¾©èˆ‡ç®¡ç†
â”‚   â”œâ”€â”€ data_management/          # è³‡æ–™ç®¡ç†æ¨¡çµ„
â”‚   â”‚   â”œâ”€â”€ data_validator.py     # è³‡æ–™é©—è­‰èˆ‡æ¸…ç†
â”‚   â”‚   â”œâ”€â”€ dataset_analyzer.py   # æ¨™ç±¤åˆ†å¸ƒåˆ†æ
â”‚   â”‚   â””â”€â”€ version_manager.py    # è³‡æ–™ç‰ˆæœ¬æ§åˆ¶
â”‚   â”œâ”€â”€ logger_config.py          # æ—¥èªŒç³»çµ±
â”‚   â””â”€â”€ train_lora_v2.py         # LoRA è¨“ç·´ä¸»ç¨‹å¼
â”œâ”€â”€ config/
â”‚   â””â”€â”€ default.yaml              # é è¨­é…ç½®æ–‡ä»¶
â”œâ”€â”€ results/                       # å¯¦é©—çµæœç›®éŒ„
â”‚   â””â”€â”€ {å¯¦é©—åç¨±}_{æ™‚é–“æˆ³}/      # ç¨ç«‹å¯¦é©—ç›®éŒ„
â”‚       â”œâ”€â”€ logs.txt            # ç³»çµ±æ—¥èªŒèˆ‡è¨“ç·´é€²åº¦
â”‚       â”œâ”€â”€ config.yaml         # å¯¦é©—é…ç½®
â”‚       â”œâ”€â”€ metrics.json        # è©•ä¼°æŒ‡æ¨™
â”‚       â””â”€â”€ artifacts/          # æ¨¡å‹èˆ‡å…¶ä»–ç”¢å‡º
â”‚           â””â”€â”€ final_model/    # è¨“ç·´å®Œæˆçš„æ¨¡å‹
â”œâ”€â”€ requirements.txt              # ä¾è³´ç®¡ç†
â”œâ”€â”€ Makefile                      # ç°¡åŒ–æŒ‡ä»¤
â””â”€â”€ README.md
```

---

## ğŸš€ å¿«é€Ÿé–‹å§‹

### åŸºæœ¬ä½¿ç”¨

```bash
make setup-conda   # å»ºç«‹ Conda ç’°å¢ƒï¼ˆè‡ªå‹•åµæ¸¬ GPU/MPS/CPUï¼‰
make run-local     # ä½¿ç”¨é è¨­é…ç½®é–‹å§‹è¨“ç·´
make logs-local    # æŸ¥çœ‹æœ€æ–°å¯¦é©—çš„è¨“ç·´é€²åº¦
```

### è‡ªå®šç¾©è¨“ç·´

1. **ä¿®æ”¹é è¨­é…ç½®**ï¼š
   ç›´æ¥ç·¨è¼¯ `config/default.yaml`

2. **ä½¿ç”¨å‘½ä»¤åˆ—åƒæ•¸**ï¼š
   ```bash
   python app/train_lora_v2.py \
     --experiment_name "custom_test" \
     --learning_rate 0.001 \
     --epochs 3 \
     --train_samples 1000
   ```

### å¸¸ç”¨åƒæ•¸

```yaml
# åœ¨ config/default.yaml ä¸­å¯èª¿æ•´ï¼š

model:
  name: "distilbert-base-uncased"
  num_labels: 2

training:
  learning_rate: 5.0e-4
  num_train_epochs: 1
  per_device_train_batch_size: 2

lora:
  r: 8
  lora_alpha: 16
  target_modules: ["q_lin", "v_lin"]
  lora_dropout: 0.1
```

---

## ğŸ“Š å¯¦é©—è¨˜éŒ„

æ¯æ¬¡è¨“ç·´æœƒè‡ªå‹•å‰µå»ºå¯¦é©—å°ˆå±¬ç›®éŒ„ï¼š`results/{å¯¦é©—åç¨±}_{æ™‚é–“æˆ³}/`

```
results/
â””â”€â”€ experiment_name_20240101_120000/
    â”œâ”€â”€ logs.txt           # ç³»çµ±æ—¥èªŒèˆ‡è¨“ç·´é€²åº¦
    â”œâ”€â”€ config.yaml        # æœ¬æ¬¡å¯¦é©—çš„å®Œæ•´é…ç½®
    â”œâ”€â”€ metrics.json       # è¨“ç·´çµæœèˆ‡è©•ä¼°æŒ‡æ¨™
    â””â”€â”€ artifacts/         # æ¨¡å‹èˆ‡å…¶ä»–ç”¢å‡º
        â””â”€â”€ final_model/   # è¨“ç·´å®Œæˆçš„æ¨¡å‹
```

- **ç³»çµ±æ—¥èªŒ**ï¼šè¨˜éŒ„è¨­å‚™ã€æ¨¡å‹è¼‰å…¥ã€è³‡æ–™è™•ç†ç­‰ç³»çµ±æ“ä½œ
- **è¨“ç·´é€²åº¦**ï¼šè¨˜éŒ„æ¯å€‹æ­¥é©Ÿçš„æå¤±å€¼ã€å­¸ç¿’ç‡ã€è©•ä¼°æŒ‡æ¨™ç­‰
- **å¯¦é©—é…ç½®**ï¼šåŒ…å«æ‰€æœ‰åƒæ•¸è¨­å®šï¼Œç¢ºä¿å¯¦é©—å¯é‡ç¾
- **è©•ä¼°æŒ‡æ¨™**ï¼šä¿å­˜æœ€çµ‚çš„è¨“ç·´æ™‚é–“ã€æº–ç¢ºç‡ç­‰çµæœ

---

## ğŸ”§ è³‡æ–™ç®¡ç†å·¥å…·

ä»¥ä¸‹æŒ‡ä»¤ä½¿ç”¨é è¨­çš„ SST-2 ç¯„ä¾‹è³‡æ–™é›†ï¼Œåƒ…ä¾›é–‹ç™¼æ¸¬è©¦ç”¨é€”ã€‚
å¯¦éš›è¨“ç·´æ™‚ï¼Œé€™äº›åŠŸèƒ½å·²æ•´åˆåœ¨è¨“ç·´æµç¨‹ä¸­è‡ªå‹•åŸ·è¡Œã€‚

```bash
make data-analyze    # åˆ†ææ¨™ç±¤åˆ†å¸ƒ
make data-validate   # é©—è­‰è³‡æ–™å“è³ª
make data-versions   # ç®¡ç†è³‡æ–™ç‰ˆæœ¬
```

**è³‡æ–™é©—è­‰å ±å‘Šç¯„ä¾‹**ï¼š
```json
{
  "total_samples": 500,
  "label_counts": {"0": 245, "1": 255},
  "imbalance_ratio": 1.04,
  "is_balanced": true
}
```

---

## ğŸ’¡ æ³¨æ„äº‹é …

- é¦–æ¬¡ä½¿ç”¨è«‹åŸ·è¡Œ `make setup-conda` è¨­ç½®ç’°å¢ƒ
- ä½¿ç”¨ `make help` æŸ¥çœ‹å®Œæ•´çš„å‘½ä»¤èªªæ˜
- å¯¦é©—é…ç½®æœƒè‡ªå‹•ä¿å­˜ï¼Œæ–¹ä¾¿è¿½è¹¤å’Œé‡ç¾
- è³‡æ–™ç®¡ç†åŠŸèƒ½åœ¨è¨“ç·´æ™‚è‡ªå‹•åŸ·è¡Œï¼Œç¢ºä¿è³‡æ–™å“è³ª