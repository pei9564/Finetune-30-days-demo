# Finetune-30-days â€” LoRA è¨“ç·´ç’°å¢ƒ

æ­¤ç›®éŒ„æä¾›ä¸€å€‹æœ€å°åŒ–çš„ **LoRA å¾®èª¿ç¯„ä¾‹**ï¼Œæ”¯æ´ M3 æ™¶ç‰‡ (MPS)ã€NVIDIA GPU (CUDA) ä»¥åŠ CPUã€‚
è¨­è¨ˆç›®æ¨™ï¼šå¿«é€Ÿå»ºç«‹ç’°å¢ƒã€é©—è­‰æµç¨‹ã€ä¿å­˜çµæœã€‚

---

## ğŸ“‚ å°ˆæ¡ˆçµæ§‹

```
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ train_lora.py      # LoRA è¨“ç·´è…³æœ¬
â”‚   â””â”€â”€ logger_config.py   # æ—¥èªŒç³»çµ±
â”œâ”€â”€ results/               # è¨“ç·´è¼¸å‡º
â”œâ”€â”€ logs/                  # è¨“ç·´æ—¥èªŒ
â”œâ”€â”€ requirements.txt       # ä¾è³´ç®¡ç†
â”œâ”€â”€ Makefile               # ç°¡åŒ–æŒ‡ä»¤
â””â”€â”€ README.md
```

---

## ğŸš€ å¿«é€Ÿé–‹å§‹

### ä¸€éµåŸ·è¡Œï¼ˆæ¨è–¦ï¼‰

```bash
make setup-conda   # è‡ªå‹•å»ºç«‹ Conda ç’°å¢ƒï¼ˆä¾ç¡¬é«”åˆ¤æ–· GPU/MPS/CPUï¼‰
make run-local     # é–‹å§‹è¨“ç·´
```

### åˆ†æ­¥é©Ÿ

```bash
brew install --cask miniforge   # å®‰è£ Conda
make setup-conda                # å»ºç«‹ç’°å¢ƒ
make run-local                  # å•Ÿå‹•è¨“ç·´
```

---

## âš™ï¸ å¯èª¿åƒæ•¸

åœ¨ `app/train_lora.py` å¯ä¿®æ”¹ï¼š

```python
num_train_epochs = 1
learning_rate = 5e-4
per_device_train_batch_size = 2
logging_steps = 10

lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["q_lin", "v_lin"],
    lora_dropout=0.1,
    task_type="SEQ_CLS"
)
```

---

## ğŸ“Š è¨“ç·´ç›£æ§

* çµ‚ç«¯ï¼šå³æ™‚é€²åº¦æ¢èˆ‡ loss/acc
* æª”æ¡ˆï¼šè©³ç´°æ—¥èªŒè¼¸å‡ºè‡³ `logs/local_training.log`
* çµæœï¼šæ¨¡å‹èˆ‡æŒ‡æ¨™ä¿å­˜åœ¨ `results/`

---

## âœ… ä½¿ç”¨æµç¨‹

1. `make setup-conda` â€” å»ºç«‹ç’°å¢ƒ
2. `make run-local` â€” å•Ÿå‹•è¨“ç·´
3. `make logs-local` â€” æŸ¥çœ‹æœ€å¾Œ 20 è¡Œæ—¥èªŒ
4. æŸ¥çœ‹ `results/` â€” åˆ†ææ¨¡å‹è¼¸å‡º

---

## ğŸ¯ å®šä½

é€™æ˜¯ä¸€å€‹ **å°ˆæ¡ˆèµ·é»**ï¼Œç›®çš„åœ¨æ–¼ï¼š

* ç¢ºèª LoRA è¨“ç·´æµç¨‹å¯åœ¨ä¸åŒç¡¬é«”ç’°å¢ƒæ­£å¸¸é‹è¡Œ
* å»ºç«‹å¾ŒçºŒå¾®èª¿å¹³å°é–‹ç™¼çš„åŸºç¤
