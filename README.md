# Finetune-30-days â€” LoRA è¨“ç·´èˆ‡è³‡æ–™ç®¡ç†

æ­¤å°ˆæ¡ˆæä¾›ä¸€å€‹æœ€å°åŒ–çš„ **LoRA å¾®èª¿ç¯„ä¾‹**ï¼Œæ”¯æ´ M3 æ™¶ç‰‡ (MPS)ã€NVIDIA GPU (CUDA) èˆ‡ CPUã€‚
è¨­è¨ˆç›®æ¨™ï¼šå¿«é€Ÿå»ºç«‹ç’°å¢ƒã€é©—è­‰æµç¨‹ã€ä¿å­˜çµæœï¼Œä¸¦å…·å‚™ **è³‡æ–™ç‰ˆæœ¬ç®¡ç†èˆ‡é©—è­‰æ©Ÿåˆ¶**ã€‚

---

## ğŸ“‚ å°ˆæ¡ˆçµæ§‹

```
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ data_management/             # è³‡æ–™ç®¡ç†æ¨¡çµ„
â”‚   â”‚   â”œâ”€â”€ data_validator.py        # è³‡æ–™é©—è­‰èˆ‡æ¸…ç† (ç©ºå€¼ / é‡è¤‡ / HTML)
â”‚   â”‚   â”œâ”€â”€ dataset_analyzer.py      # æ¨™ç±¤åˆ†å¸ƒèˆ‡ä¸å¹³è¡¡åˆ†æ
â”‚   â”‚   â””â”€â”€ version_manager.py       # è³‡æ–™ç‰ˆæœ¬æ§åˆ¶èˆ‡ metadata
â”‚   â”œâ”€â”€ logger_config.py             # æ—¥èªŒç³»çµ±
â”‚   â””â”€â”€ train_lora.py                # LoRA è¨“ç·´ä¸»ç¨‹å¼
â”œâ”€â”€ results/                         # è¨“ç·´è¼¸å‡º (æ¨¡å‹ / è©•ä¼°å ±å‘Š)
â”œâ”€â”€ logs/                            # è¨“ç·´èˆ‡é©—è­‰æ—¥èªŒ
â”œâ”€â”€ requirements.txt                 # ä¾è³´ç®¡ç†
â”œâ”€â”€ Makefile                         # ç°¡åŒ–æŒ‡ä»¤ (è¨“ç·´ / è³‡æ–™æª¢æŸ¥)
â””â”€â”€ README.md
```

---

## ğŸš€ å¿«é€Ÿé–‹å§‹

### ä¸€éµåŸ·è¡Œ

```bash
make setup-conda   # å»ºç«‹ Conda ç’°å¢ƒï¼ˆè‡ªå‹•åµæ¸¬ GPU/MPS/CPUï¼‰
make run-local     # å•Ÿå‹•è¨“ç·´
```

### åˆ†æ­¥é©Ÿ

```bash
brew install --cask miniforge
make setup-conda
make run-local
```

---

## ğŸ“Š è³‡æ–™ç®¡ç†

å°ˆæ¡ˆå…§å»º **ç‰ˆæœ¬ç®¡ç† / åˆ†å¸ƒæª¢æŸ¥ / å“è³ªé©—è­‰**ï¼Œä¸¦åœ¨ LoRA è¨“ç·´æµç¨‹ä¸­è‡ªå‹•åŸ·è¡Œã€‚
ä»¥ä¸‹æŒ‡ä»¤åƒ…ä½œç‚º **å–®ç¨æ¸¬è©¦ç¯„ä¾‹**ï¼š

```bash
make data-versions   # å»ºç«‹è³‡æ–™ç‰ˆæœ¬
make data-analyze    # æ¨™ç±¤åˆ†å¸ƒåˆ†æ
make data-validate   # æª¢æŸ¥ç©ºå€¼ / é‡è¤‡ / é•·åº¦ / HTML
```

**è¼¸å‡ºç¯„ä¾‹**

```json
{
  "total_samples": 500,
  "label_counts": {"0": 245, "1": 255},
  "imbalance_ratio": 1.04,
  "is_balanced": true
}
```

å¯¦éš›ä½¿ç”¨æ™‚ï¼Œé€™äº›æ­¥é©Ÿæœƒåœ¨ `make run-local` åŸ·è¡Œè¨“ç·´æ™‚ **è‡ªå‹•å¥—ç”¨åˆ°è³‡æ–™å‰è™•ç†**ã€‚
å› æ­¤æ¯ä¸€æ¬¡è¨“ç·´éƒ½èƒ½ä¿è­‰ï¼š

* æœ‰è³‡æ–™ç‰ˆæœ¬è¨˜éŒ„
* æœ‰æ¨™ç±¤åˆ†å¸ƒå ±å‘Š
* å·²å®ŒæˆåŸºç¤è³‡æ–™æ¸…ç†

---

## âš™ï¸ å¯èª¿åƒæ•¸

åœ¨ `app/train_lora.py` å¯ä¿®æ”¹ï¼š

```python
num_train_epochs = 1
learning_rate = 5e-4
per_device_train_batch_size = 2
```

LoRA ç›¸é—œè¨­å®šï¼š

```python
lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["q_lin", "v_lin"],
    lora_dropout=0.1,
    task_type="SEQ_CLS"
)
```

---

## âœ… ä½¿ç”¨æµç¨‹

1. `make setup-conda` â€” å»ºç«‹ç’°å¢ƒ
2. `make data-validate` â€” é©—è­‰è³‡æ–™å“è³ª
3. `make run-local` â€” å•Ÿå‹•è¨“ç·´
4. æŸ¥çœ‹ `results/` â€” åˆ†ææ¨¡å‹è¼¸å‡º

---

é€™æ˜¯ä¸€å€‹ **Day4 çš„å°ˆæ¡ˆåŸºç¤ç‰ˆ**ï¼Œç›®çš„åœ¨æ–¼ï¼š

* ç¢ºèª LoRA è¨“ç·´å¯åœ¨ä¸åŒç¡¬é«”æ­£å¸¸é‹è¡Œ
* å…·å‚™è³‡æ–™ç‰ˆæœ¬åŒ–èˆ‡é©—è­‰ï¼Œç¢ºä¿å¯¦é©—å¯è¿½æº¯
* ç‚ºå¾ŒçºŒä»»å‹™æ’ç¨‹ã€åƒæ•¸ç®¡ç†ã€æ¨¡å‹è©•ä¼°åšå¥½åŸºç¤
